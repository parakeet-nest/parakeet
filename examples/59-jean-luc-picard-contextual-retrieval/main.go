package main

/*
https://www.anthropic.com/news/contextual-retrieval
*/

import (
	"fmt"
	"log"
	"strconv"

	"github.com/parakeet-nest/parakeet/cli"
	"github.com/parakeet-nest/parakeet/completion"
	"github.com/parakeet-nest/parakeet/content"
	"github.com/parakeet-nest/parakeet/embeddings"
	"github.com/parakeet-nest/parakeet/enums/option"
	"github.com/parakeet-nest/parakeet/llm"
	"github.com/parakeet-nest/parakeet/ui"
	"github.com/parakeet-nest/parakeet/ui/colors"
)

func createEmbeddings(docsPath, storePath, ollamaUrl, embeddingsModel, contextualModel string) {

	store := embeddings.BboltVectorStore{}
	err := store.Initialize(storePath)
	if err != nil {
		log.Fatalln("ðŸ˜¡:", err)
	}

	content.ForEachFile(docsPath, ".md", func(documentPath string) error {
		fmt.Println("ðŸ“ Creating embedding from document ", documentPath)

		// Read the content of the file
		wholeDocumentContent, err := content.ReadTextFile(documentPath)
		if err != nil {
			log.Fatalln("ðŸ˜¡:", err)
		}

		chunks := content.ParseMarkdownWithLineage(wholeDocumentContent)

		fmt.Println("ðŸ‘‹ Found", len(chunks), "chunks")

		// Create embeddings from documents and save them in the store
		for idx, doc := range chunks {
			fmt.Println("ðŸ“ Creating embedding from document ", idx)
			fmt.Println("ðŸ–¼ï¸", doc.Header)

			// better chunk embedding
			options := llm.SetOptions(map[string]interface{}{
				option.Temperature: 0.8,
			})

			context, err := content.CreateChunkContext(wholeDocumentContent, doc, ollamaUrl, contextualModel, options)
			if err != nil {
				log.Println("ðŸ˜¡:", err)
			}
			ui.Println(colors.Green, "---[Chunk context]--------------------------------")
			ui.Println(colors.Green, context)
			ui.Println(colors.Green, "--------------------------------------------------")

			embedding, err := embeddings.CreateEmbedding(
				ollamaUrl,
				llm.Query4Embedding{
					Model:  embeddingsModel,
					Prompt: context + "\n" + fmt.Sprintf("## %s\n\n%s\n\n", doc.Header, doc.Content),
					//Prompt: fmt.Sprintf("## %s\n\n%s\n\n", doc.Header, doc.Content),
				},
				documentPath+"-"+strconv.Itoa(idx),
			)
			if err != nil {
				fmt.Println("ðŸ˜¡:", err)
			} else {
				//embedding.MetaData = "ðŸ“ chunk num: " + strconv.Itoa(idx)
				_, err := store.Save(embedding)
				if err != nil {
					fmt.Println("ðŸ˜¡:", err)
				}
			}

			ui.Println(colors.Yellow, "---[Improved chunk]--------------------------------")
			ui.Println(colors.Yellow, embedding.Prompt)
			ui.Println(colors.Yellow, "---------------------------------------------------")

		}

		return nil
	})

}

func chatWithCharacter(storePath, ollamaUrl, embeddingsModel, chatModel, instructionsPath, contextPath string) {
	store := embeddings.BboltVectorStore{}
	err := store.Initialize(storePath)
	if err != nil {
		log.Fatalln("ðŸ˜¡:", err)
	}

	// Read the content of the file
	instructions, err := content.ReadTextFile(instructionsPath)
	if err != nil {
		log.Fatalln("ðŸ˜¡:", err)
	}
	context, err := content.ReadTextFile(contextPath)
	if err != nil {
		log.Fatalln("ðŸ˜¡:", err)
	}
	options := llm.SetOptions(map[string]interface{}{
		option.Temperature:   0.0,
		option.RepeatLastN:   2,
		option.RepeatPenalty: 3.0,
		option.TopK:          10,
		option.TopP:          0.5,
	})

	memory := []llm.Message{}

	for {

		question, _ := ui.Input(colors.Cyan, fmt.Sprintf("ðŸ¤– [%s] ask me something> ", chatModel))

		if question == "bye" {
			break
		}

		// Create an embedding from the question
		embeddingFromQuestion, err := embeddings.CreateEmbedding(
			ollamaUrl,
			llm.Query4Embedding{
				Model:  embeddingsModel,
				Prompt: question,
			},
			"question",
		)
		if err != nil {
			log.Fatalln("ðŸ˜¡:", err)
		}
		fmt.Println("ðŸ”Ž searching for similarity...")

		//similarities, _ := store.SearchSimilarities(embeddingFromQuestion, 0.3)
		similarities, _ := store.SearchTopNSimilarities(embeddingFromQuestion, 0.3, 2)

		fmt.Println("ðŸŽ‰ number of similarities:", len(similarities))

		documentsContent := embeddings.GenerateContextFromSimilarities(similarities)

		fmt.Println(documentsContent)

		messages := []llm.Message{
			{Role: "system", Content: instructions},
			{Role: "system", Content: context}, // ex: personality
			{Role: "system", Content: documentsContent},
			//{Role: "user", Content: question},
		}

		messages = append(messages, memory...)
		messages = append(messages, llm.Message{Role: "user", Content: question})

		queryChat := llm.Query{
			Model:    chatModel,
			Messages: messages,
			Options:  options,
		}

		fmt.Println()
		ui.Println(colors.Magenta, "ðŸ¤– answer:")

		// Answer the question
		lastAnswer, err := completion.ChatStream(ollamaUrl, queryChat,
			func(answer llm.Answer) error {
				fmt.Print(answer.Message.Content)
				return nil
			})

		if err != nil {
			log.Fatal("ðŸ˜¡:", err)
		}

		// Save the conversation in memory
		memory = append(
			memory,
			llm.Message{Role: "user", Content: question},
			llm.Message{Role: "assistant", Content: lastAnswer.Message.Content},
		)

		// Then, if the length is greater than 2, remove the two oldest messages
		if len(memory) > 2 {
			memory = memory[2:]
		}

		fmt.Println()

		//ui.Println(colors.Orange, answer.Message.Content)
	}

}

/*

go run main.go create-embeddings \
--embeddings-model bge-m3:latest \
--contextual-model qwen2.5:3b \
--docs-path ./docs \
--store-path ./embeddings.db \
--url http://localhost:11434


go run main.go chat \
--embeddings-model bge-m3:latest \
--chat-model qwen2.5:0.5b \
--store-path ./embeddings.db \
--context-path ./context.md \
--instructions-path ./instructions.md \
--url http://localhost:11434

--- with a RPi ---

go run main.go create-embeddings \
--embeddings-model mxbai-embed-large:latest  \
--contextual-model qwen2.5:1.5b \
--docs-path ./docs \
--store-path ./embeddings.db \
--url http://bob.local:11434

--url http://localhost:11434


go run main.go chat \
--embeddings-model mxbai-embed-large:latest \
--chat-model qwen2.5:0.5b \
--store-path ./embeddings.db \
--context-path ./context.md \
--instructions-path ./instructions.md \
--url http://bob.local:11434

--url http://localhost:11434

*/

/*
- add flags for a config file (or string) for options
- number of top similarities
- verbose mode
- set the number of messages to keep in memory

*/

func main() {

	// default values
	ollamaUrl := "http://localhost:11434"
	embeddingsModel := "bge-m3:latest"
	storePath := "./embeddings.db"
	docsPath := "./docs"
	contextualModel := "tinydolphin"

	chatModel := "llama3.1:8b"
	contextPath := "./context.md"
	instructionsPath := "./instructions.md"

	args, flags := cli.Settings()

	if cli.HasFlag("url", flags) {
		ollamaUrl = cli.FlagValue("url", flags)
	}
	if cli.HasFlag("embeddings-model", flags) {
		embeddingsModel = cli.FlagValue("embeddings-model", flags)
	}
	if cli.HasFlag("store-path", flags) {
		storePath = cli.FlagValue("store-path", flags)
	}
	if cli.HasFlag("docs-path", flags) {
		docsPath = cli.FlagValue("docs-path", flags)
	}
	if cli.HasFlag("contextual-model", flags) {
		contextualModel = cli.FlagValue("contextual-model", flags)
	}

	if cli.HasFlag("chat-model", flags) {
		chatModel = cli.FlagValue("chat-model", flags)
	}
	if cli.HasFlag("context-path", flags) {
		contextPath = cli.FlagValue("context-path", flags)
	}
	if cli.HasFlag("instructions-path", flags) {
		instructionsPath = cli.FlagValue("instructions-path", flags)
	}

	switch cmd := cli.ArgsTail(args); cmd[0] {
	case "create-embeddings":
		createEmbeddings(docsPath, storePath, ollamaUrl, embeddingsModel, contextualModel)
	case "chat":
		chatWithCharacter(storePath, ollamaUrl, embeddingsModel, chatModel, instructionsPath, contextPath)
	default:
		fmt.Println("Unknown command:", cmd[0])
	}

}
