{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe0 Home","text":""},{"location":"#parakeet","title":"\ud83e\udd9c\ud83e\udeba Parakeet","text":"<p>Parakeet is the simplest Go library to create GenAI apps with Ollama.</p> <p>A GenAI app is an application that uses generative AI technology. Generative AI can create new text, images, or other content based on what it's been trained on. So a GenAI app could help you write a poem, design a logo, or even compose a song! These are still under development, but they have the potential to be creative tools for many purposes. - Gemini</p> <p>\u270b Parakeet is only for creating GenAI apps generating text (not image, music,...).</p>"},{"location":"#install","title":"Install","text":"<p>Note</p> <p>current release: <code>v0.2.9 \ud83e\udd67 [pie]</code></p> <pre><code>go get github.com/parakeet-nest/parakeet\n</code></pre>"},{"location":"#getting-started-first-completion","title":"\ud83d\ude80 Getting Started - First completion","text":"<p><code>generate</code></p> <p>The simple completion can be used to generate a response for a given prompt with a provided model.</p> <pre><code>package main\n\nimport (\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n    \"github.com/parakeet-nest/parakeet/enums/option\"\n\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"tinydolphin\"\n\n    options := llm.SetOptions(map[string]interface{}{\n        option.Temperature: 0.5,\n    })\n\n    question := llm.GenQuery{\n        Model: model,\n        Prompt: \"Who is James T Kirk?\",\n        Options: options,\n    }\n\n    answer, err := completion.Generate(ollamaUrl, question)\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n    fmt.Println(answer.Response)\n}\n</code></pre>"},{"location":"chat-completion/","title":"\ud83d\udcac Chat completion","text":""},{"location":"chat-completion/#chat-completion","title":"Chat completion","text":""},{"location":"chat-completion/#completion","title":"Completion","text":"<p>The chat completion can be used to generate a conversational response for a given set of messages with a provided model.</p> <pre><code>package main\n\nimport (\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n    \"github.com/parakeet-nest/parakeet/enums/option\"\n\n\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"deepseek-coder\"\n\n    systemContent := `You are an expert in computer programming.\n    Please make friendly answer for the noobs.\n    Add source code examples if you can.`\n\n    userContent := `I need a clear explanation regarding the following question:\n    Can you create a \"hello world\" program in Golang?\n    And, please, be structured with bullet points`\n\n    options := llm.SetOptions(map[string]interface{}{\n        option.Temperature: 0.5,\n        option.RepeatLastN: 2,\n        option.RepeatPenalty: 2.0,\n    })\n\n    query := llm.Query{\n        Model: model,\n        Messages: []llm.Message{\n            {Role: \"system\", Content: systemContent},\n            {Role: \"user\", Content: userContent},\n        },\n        Options: options,\n        Stream: false,\n    }\n\n    answer, err := completion.Chat(ollamaUrl, query)\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n    fmt.Println(answer.Message.Content)\n}\n</code></pre> <p>\u270b To keep a conversational memory for the next chat completion, update the list of messages with the previous question and answer.</p>"},{"location":"chat-completion/#completion-with-stream","title":"Completion with stream","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"deepseek-coder\"\n\n    systemContent := `You are an expert in computer programming.\n    Please make friendly answer for the noobs.\n    Add source code examples if you can.`\n\n    userContent := `I need a clear explanation regarding the following question:\n    Can you create a \"hello world\" program in Golang?\n    And, please, be structured with bullet points`\n\n    options := llm.Options{\n        Temperature: 0.5,\n        RepeatLastN: 2, \n    }\n\n    query := llm.Query{\n        Model: model,\n        Messages: []llm.Message{\n            {Role: \"system\", Content: systemContent},\n            {Role: \"user\", Content: userContent},\n        },\n        Options: options,\n        Stream:  false,\n    }\n\n    _, err := completion.ChatStream(ollamaUrl, query,\n        func(answer llm.Answer) error {\n            fmt.Print(answer.Message.Content)\n            return nil\n        })\n\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n}\n</code></pre>"},{"location":"chat-completion/#chat-completion-with-conversational-memory","title":"Chat completion with conversational memory","text":""},{"location":"chat-completion/#in-memory-history","title":"In memory history","text":"<p>To store the messages in memory, use <code>history.MemoryMessages</code></p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/history\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"tinydolphin\" // fast, and perfect answer (short, brief)\n\n    conversation := history.MemoryMessages{\n        Messages: make(map[string]llm.MessageRecord),\n    }\n\n    systemContent := `You are an expert with the Star Trek series. use the history of the conversation to answer the question`\n\n    userContent := `Who is James T Kirk?`\n\n    options := llm.Options{\n        Temperature: 0.5,\n        RepeatLastN: 2,  \n    }\n\n    query := llm.Query{\n        Model: model,\n        Messages: []llm.Message{\n            {Role: \"system\", Content: systemContent},\n            {Role: \"user\", Content: userContent},\n        },\n        Options: options,\n    }\n\n    // Ask the question\n    answer, err := completion.ChatStream(ollamaUrl, query,\n        func(answer llm.Answer) error {\n            fmt.Print(answer.Message.Content)\n            return nil\n        },\n    )\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n\n    // Save the conversation\n    _, err = conversation.SaveMessage(\"1\", llm.Message{\n        Role:    \"user\",\n        Content: userContent,\n    })\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n\n    _, err = conversation.SaveMessage(\"2\", llm.Message{\n        Role:    \"system\",\n        Content: answer.Message.Content,\n    })\n\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n\n    // New question\n    userContent = `Who is his best friend ?`\n\n    previousMessages, _ := conversation.GetAllMessages()\n\n    // (Re)Create the conversation\n    conversationMessages := []llm.Message{}\n    // instruction\n    conversationMessages = append(conversationMessages, llm.Message{Role: \"system\", Content: systemContent})\n    // history\n    conversationMessages = append(conversationMessages, previousMessages...)\n    // last question\n    conversationMessages = append(conversationMessages, llm.Message{Role: \"user\", Content: userContent})\n\n    query = llm.Query{\n        Model:    model,\n        Messages: conversationMessages,\n        Options:  options,\n    }\n\n    answer, err = completion.ChatStream(ollamaUrl, query,\n        func(answer llm.Answer) error {\n            fmt.Print(answer.Message.Content)\n            return nil\n        },\n    )\n    fmt.Println()\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n\n}\n</code></pre>"},{"location":"chat-completion/#bbolt-history","title":"Bbolt history","text":"<p>Bbolt is an embedded key/value database for Go.</p> <p>To store the messages in a bbolt bucket, use <code>history.BboltMessages</code></p> <pre><code>conversation := history.BboltMessages{}\nconversation.Initialize(\"../conversation.db\")\n</code></pre> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/11-chat-conversational-bbolt</li> <li>examples/11-chat-conversational-bbolt/begin: start a conversation and save the history</li> <li>examples/11-chat-conversational-bbolt/resume: load the messages from the history bucket and resue the conversation</li> </ul>"},{"location":"chat-completion/#conversational-history-remove-messages","title":"Conversational history: remove messages","text":""},{"location":"chat-completion/#in-memory","title":"In Memory","text":"<ul> <li>Remove a message by id <code>history.RemoveMessage(id string)</code></li> </ul> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/69-web-chat-bot</li> </ul>"},{"location":"chat-completion/#bbolt-memory","title":"Bbolt Memory","text":"<ul> <li>Remove a message by id <code>history.RemoveMessage(id string)</code></li> </ul>"},{"location":"chat-completion/#conversational-history-handling-sessions","title":"Conversational history: handling sessions","text":""},{"location":"chat-completion/#in-memory_1","title":"In Memory","text":"<ul> <li><code>history.SaveMessageWithSession(sessionId, messageId string, message llm.Message)</code></li> <li><code>history.RemoveTopMessageOfSession(sessionId string)</code></li> </ul> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/70-web-chat-bot-with-session</li> </ul>"},{"location":"chat-completion/#bbolt-memory_1","title":"Bbolt Memory","text":"<ul> <li><code>history.SaveMessageWithSession(sessionId, messageId string, message llm.Message)</code></li> <li><code>history.RemoveTopMessageOfSession(sessionId string)</code></li> </ul> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/71-web-chat-bot-with-session</li> </ul>"},{"location":"chat-completion/#in-memory-and-bbolt","title":"In Memory and Bbolt","text":"<ul> <li><code>history.RemoveTopMessage() error</code>: removes the oldest message from the Messages list.</li> <li><code>history.KeepLastN(n int) error</code>: keeps the last n messages in the Messages list (and remove the oldest).</li> <li><code>history.KeepLastNOfSession(sessionId string, n int) error</code>: keeps the last n messages of the session in the Messages list (and remove the oldest).</li> <li><code>history.GetLastNMessages(n int) ([]llm.Message, error)</code>: returns the last n messages in the Messages list.</li> </ul> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/69-web-chat-bot</li> <li>examples/70-web-chat-bot-with-session</li> <li>examples/71-web-chat-bot-with-session</li> <li>examples/72-gitingest-es</li> <li>examples/73-gitingest-daphnia</li> <li>examples/84-conversational-memory</li> <li>examples/85-conversational-bbolt</li> </ul>"},{"location":"chat-completion/#complex-conversation","title":"Complex conversation","text":"<p>You can use the helper function <code>llm.Conversation</code>. <code>Conversation</code> creates or extends a conversation with provided messages. It can accept either single messages or slices of messages as variadic parameters:</p> <pre><code>conversationMessages := llm.Conversation(\n    llm.Message{Role: \"system\", Content: \"Enable deep thinking subroutine.\"},\n    llm.Message{Role: \"system\", Content: systemInstructions},\n    []llm.Message{\n        llm.Message{Role: \"user\", Content: question},\n        llm.Message{Role: \"assistant\", Content: assistantMessage},\n    },\n    llm.Message{Role: \"user\", Content: userMessage},\n)\n</code></pre> <p>It returns a slice of messages <code>[]llm.Message</code></p>"},{"location":"chunkers-and-splitters/","title":"Chunkers and Splitters","text":"<p>There are several methods in the <code>content</code> package to help you chunk and split text:</p> <ul> <li><code>ChunkText</code> takes a text string and divides it into chunks of a specified size with a given overlap. It returns a slice of strings, where each string represents a chunk of the original text.</li> </ul> <pre><code>chunks := content.ChunkText(documentContent, 900, 400)\n</code></pre> <ul> <li><code>SplitTextWithDelimiter</code> splits the given text using the specified delimiter and returns a slice of strings.</li> </ul> <pre><code>chunks := content.SplitTextWithDelimiter(documentContent, \"&lt;!-- SPLIT --&gt;\")\n</code></pre> <ul> <li><code>SplitTextWithRegex</code> splits the given text using the provided regular expression delimiter. It returns a slice of strings containing the split parts of the text.</li> </ul> <pre><code>chunks := content.SplitTextWithRegex(documentContent, `## *`)\n</code></pre> <ul> <li><code>SplitMarkdownBySections</code> splits the given markdown text using the title sections (<code>#, ##, etc.</code>) and returns a slice of strings.</li> </ul> <pre><code>chunks := content.SplitMarkdownBySections(documentContent)\n</code></pre> <ul> <li><code>SplitAsciiDocBySections</code> splits the given asciidoc text using the title sections (<code>=, ==, etc.</code>) and returns a slice of strings.</li> </ul> <pre><code>chunks := content.SplitAsciiDocBySections(documentContent)\n</code></pre> <ul> <li><code>SplitHTMLBySections</code> splits the given html text using the title sections (<code>h1, h2, h3, h4, h5, h6</code>) and returns a slice of strings.</li> </ul> <pre><code>chunks := content.SplitHTMLBySections(documentContent)\n</code></pre>"},{"location":"cli-helpers/","title":"CLI Helpers","text":"<p>\ud83d\udce6 <code>content</code> package</p> <p>These helpers provide methods to parse and use command-line arguments and flags.</p>"},{"location":"cli-helpers/#settings","title":"<code>Settings</code>","text":"<p><code>Settings</code> parses command-line arguments and flags.</p> <p>It skips the program name and processes the remaining arguments. Arguments that start with \"--\" are considered flags, and the function checks if the next argument is a value for the flag. If so, it pairs the flag with its value; otherwise, it pairs the flag with an empty string. Arguments that do not start with \"--\" are considered positional arguments.</p> <p>Returns two slices: one containing the positional arguments and the other containing the flags with their respective values.</p> <p>Example:</p> <pre><code>// default values\nollamaUrl := \"http://localhost:11434\"\nchatModel := \"llama3.1:8b\"\nembeddingsModel := \"bge-m3:latest\"\n\nargs, flags := cli.Settings()\n\nif cli.HasFlag(\"url\", flags) {\n    ollamaUrl = cli.FlagValue(\"url\", flags)\n}\n\nif cli.HasFlag(\"chat-model\", flags) {\n    chatModel = cli.FlagValue(\"chat-model\", flags)\n}\n\nif cli.HasFlag(\"embeddings-model\", flags) {\n    embeddingsModel = cli.FlagValue(\"embeddings-model\", flags)\n}\n\nswitch cmd := cli.ArgsTail(args); cmd[0] {\ncase \"create-embeddings\":\n    fmt.Println(embeddingsModel)\ncase \"chat\":\n    fmt.Println(chatModel)\ndefault:\n    fmt.Println(\"Unknown command:\", cmd[0])\n}\n</code></pre>"},{"location":"cli-helpers/#flagvalue","title":"<code>FlagValue</code>","text":"<p><code>FlagValue</code> retrieves the value of a flag by its name from a slice of Flag structs. If the flag is not found, it returns an empty string.</p> <p>Parameters:   - <code>name</code>: The name of the flag to search for.   - <code>flags</code>: A slice of Flag structs to search within.</p> <p>Returns:   The value of the flag if found, otherwise an empty string.</p>"},{"location":"cli-helpers/#hasarg","title":"<code>HasArg</code>","text":"<p><code>HasArg</code> checks if an argument with the specified name exists in the provided slice of arguments.</p> <p>Parameters: - <code>name</code>: The name of the argument to search for. - <code>args</code>: A slice of Arg structures to search within.</p> <p>Returns: - <code>bool</code>: True if an argument with the specified name is found, otherwise false.</p>"},{"location":"cli-helpers/#hasflag","title":"<code>HasFlag</code>","text":"<p><code>HasFlag</code> checks if a flag with the specified name exists in the provided slice of flags.</p> <p>Parameters: - <code>name</code>: The name of the flag to search for. - <code>flags</code>: A slice of Flag objects to search within.</p> <p>Returns: - <code>bool</code>: True if a flag with the specified name is found, otherwise false.</p>"},{"location":"cli-helpers/#argstail","title":"<code>ArgsTail</code>","text":"<p><code>ArgsTail</code> extracts the names from a slice of Arg structs and returns them as a slice of strings.</p> <p>Parameters: - <code>args</code>: A slice of Arg structs from which the names will be extracted.</p> <p>Returns: - A slice of strings containing the names of the provided Arg structs.</p>"},{"location":"cli-helpers/#flagstail","title":"<code>FlagsTail</code>","text":"<p><code>FlagsTail</code> takes a slice of Flag structs and returns a slice of strings containing the names of those flags.</p> <p>Parameters:   <code>flags []Flag</code>: A slice of Flag structs.</p> <p>Returns:   <code>[]string</code>: A slice of strings containing the names of the flags.</p>"},{"location":"cli-helpers/#flagswithnamestail","title":"<code>FlagsWithNamesTail</code>","text":"<p><code>FlagsWithNamesTail</code> takes a slice of Flag structs and returns a slice of strings, where each string is a formatted pair of the flag's name and value in the form \"name=value\".</p> <p>Parameters:   <code>flags []Flag</code> - A slice of Flag structs, each containing a Name and a Value.</p> <p>Returns:   <code>[]string</code> - A slice of strings, each representing a flag's name and value pair.</p>"},{"location":"cli-helpers/#hassubsequence","title":"<code>HasSubsequence</code>","text":"<p><code>HasSubsequence</code> checks if the given subsequence of strings (subSeq) is present in the tail of the provided arguments (args).</p> <p>Parameters:   - <code>args</code>: A slice of Arg representing the arguments to be checked.   - <code>subSeq</code>: A slice of strings representing the subsequence to look for.</p> <p>Returns:   - <code>bool</code>: True if the subsequence is found in the tail of the arguments, false otherwise.</p> <p>Note</p> <p>\ud83d\udc40 you will find complete examples in:</p> <ul> <li>examples/59-jean-luc-picard-contextual-retrieval</li> </ul>"},{"location":"content-helpers/","title":"Content Helpers","text":"<p>\ud83d\udea7 work in progress</p>"},{"location":"content-helpers/#context-construction","title":"Context construction","text":"<ul> <li><code>GenerateContextFromDocs</code> generates the context content from a slice of documents: <code>content.GenerateContextFromDocs(docs []string) string</code></li> </ul>"},{"location":"content-helpers/#read-and-write-files","title":"Read and Write files","text":"<ul> <li><code>content.ReadTextFile(path string) (string, error)</code></li> <li><code>content.WriteTextFile(path, content string) error</code></li> </ul>"},{"location":"content-helpers/#parsing-path","title":"Parsing path","text":"<ul> <li><code>FindFiles</code> searches for files with a specific extension in the given root directory and its subdirectories: <code>content.FindFiles(dirPath string, ext string) ([]string, error)</code></li> <li>Returns:<ul> <li>[]string: A slice of file paths that match the given extension.</li> <li>error: An error if the search encounters any issues.</li> </ul> </li> <li><code>ForEachFile</code> iterates over all files with a specific extension in a directory and its subdirectories: <code>content.ForEachFile(dirPath string, ext string, callback func(string) error) ([]string, error)</code></li> <li>Returns:<ul> <li>[]string: A slice of file paths that match the given extension.</li> <li>error: An error if the search encounters any issues.</li> </ul> </li> <li><code>GetArrayOfContentFiles</code> searches for files with a specific extension in the given directory and its subdirectories: <code>content.GetArrayOfContentFiles(dirPath string, ext string) ([]string, error)</code></li> <li><code>GetMapOfContentFiles</code> searches for files with a specific extension in the given directory and its subdirectories: <code>content.GetMapOfContentFiles(dirPath string, ext string) (map[string]string, error)</code></li> </ul>"},{"location":"content-helpers/#string-interpolation","title":"String interpolation","text":"<ul> <li><code>content.InterpolateString(str string, vars interface{}) (string, error)</code></li> </ul> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/40-rag-with-elastic-markdown</li> <li>examples/46-create-an-expert</li> </ul>"},{"location":"content-helpers/#estimate-the-number-of-tokens-in-a-text","title":"Estimate the number of tokens in a text","text":"<ul> <li><code>content.CountTokens(text string) int</code></li> <li><code>content.CountTokensAdvanced(text string) int</code></li> <li><code>content.EstimateGPTTokens(text string) int</code></li> </ul> <p>this could be useful to estimate the value of <code>num_ctx</code></p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/72-gitingest-es</li> <li>examples/73-gitingest-daphnia</li> <li>examples/74-rag-with-daphnia</li> </ul>"},{"location":"contextual-retrieval/","title":"Contextual Retrieval","text":"<p>\ud83d\udce6 <code>content</code> package</p> <p>Inspired by: Introducing Contextual Retrieval</p>"},{"location":"contextual-retrieval/#createchunkcontext","title":"<code>CreateChunkContext</code>","text":"<p><code>CreateChunkContext</code> generates a succinct context for a given chunk within the whole document content. This context is intended to improve search retrieval of the chunk.</p> <p>Parameters:   - wholeDocumentContent: The entire content of the document as a string.   - chunk: The specific chunk of the document for which context is to be generated.   - ollamaUrl: The URL for the Ollama service.   - contextualModel: The model used for generating the context.   - options: Additional options for the LLM (Language Model).</p> <p>Returns:   - A string containing the succinct context for the chunk.   - An error if the context generation fails.</p> <p><code>CreateChunkContext</code> use a default template for the prompt. If you want to use a custom template, you can use <code>CreateChunkContextWithPromptTemplate</code>.</p>"},{"location":"contextual-retrieval/#createchunkcontextwithprompttemplate","title":"<code>CreateChunkContextWithPromptTemplate</code>","text":"<p><code>CreateChunkContextWithPromptTemplate</code> generates a contextual response based on a given prompt template and document content. It interpolates the template with the provided document and chunk content, then uses an LLM to generate a response.</p> <p>Parameters:   - promptTpl: A string template for the prompt.   - wholeDocumentContent: The content of the entire document.   - chunk: A Chunk struct containing a portion of the document content.   - ollamaUrl: The URL of the LLM service.   - contextualModel: The model to be used for generating the response.   - options: Options for the LLM query.</p> <p>Returns:   - A string containing the generated response.   - An error if the process fails at any step.</p>"},{"location":"contextual-retrieval/#template-example","title":"Template example","text":"<pre><code>promptTemplateForContext := `\n&lt;chunk&gt; \n{{.chunkContent}} \n&lt;/chunk&gt; \nGenerate a brief context of the above chunk to situate this chunk within the below document. \n&lt;document&gt; \n{{.wholeDocument}} \n&lt;/document&gt; \n`\n</code></pre> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/59-jean-luc-picard-contextual-retrieval</li> </ul>"},{"location":"docker-model-runner-support/","title":"\ud83d\udc33 Docker Model Runner support","text":""},{"location":"docker-model-runner-support/#docker-model-runner-support","title":"Docker Model Runner support","text":"<p>Since the release <code>0.2.7</code> I unified the completion methods</p> <p>When you use Docker Model Runner, if - the application runs outside a container, use the following url: <code>http://localhost:12434/engines/llama.cpp/v1</code> - the application runs into a container, use the following url: <code>http://model-runner.docker.internal/engines/llama.cpp/v1/</code></p>"},{"location":"docker-model-runner-support/#chat-completion","title":"Chat completion","text":"<p>Use: <code>completion.Chat(modelRunnerURL, query, provider.DockerModelRunner)</code></p> <pre><code>modelRunnerURL := \"http://localhost:12434/engines/llama.cpp/v1\"\nmodel := \"ai/qwen2.5:latest\" \n\nsystemContent := `You are an expert in Star Trek.`\nuserContent := `Who is Jean-Luc Picard?`\n\noptions := llm.SetOptions(map[string]interface{}{\n    option.Temperature: 0.5,\n    option.RepeatPenalty: 2.0,\n})\n\nquery := llm.Query{\n    Model: model,\n    Messages: []llm.Message{\n        {Role: \"system\", Content: systemContent},\n        {Role: \"user\", Content: userContent},\n    },\n    Options: options,\n}\n\nanswer, err := completion.Chat(modelRunnerURL, query, provider.DockerModelRunner)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\nfmt.Println(answer.Message.Content)\n</code></pre>"},{"location":"docker-model-runner-support/#chat-completion-with-stream","title":"Chat completion with stream","text":"<p>Use: <code>completion.ChatStream(openAIURL, query, function, provider.DockerModelRunner)</code></p> <pre><code>options := llm.SetOptions(map[string]interface{}{\n    option.Temperature:   0.5,\n    option.RepeatPenalty: 3.0,\n})\n\nquery := llm.Query{\n    Model: \"ai/mistral:latest\",\n    Messages: []llm.Message{\n        {Role: \"system\", Content: `You are a Borg in Star Trek. Speak like a Borg`},\n        {Role: \"user\", Content: `Who is Jean-Luc Picard?`},\n    },\n    Options: options,\n}\n\n_, err := completion.ChatStream(\"http://localhost:12434/engines/llama.cpp/v1\", query,\n    func(answer llm.Answer) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    }, provider.DockerModelRunner)\n\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n</code></pre> <p>Note</p> <p>You can find examples in </p> <ul> <li>examples/77-chat-model-runner</li> <li>examples/78-chat-stream-model-runner</li> </ul>"},{"location":"docker-model-runner-support/#chat-completion-with-tools","title":"Chat completion with tools","text":"<p>Use: <code>completion.Chat(openAIURL, query, provider.DockerModelRunner) and set</code>query.Tools`.</p> <pre><code>modelRunnerURL := \"http://localhost:12434/engines/llama.cpp/v1\"\nmodel := \"ai/smollm2\"\n\ntoolsList := []llm.Tool{\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"addNumbers\",\n            Description: \"Make an addition of the two given numbers\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"a\": {\n                        Type:        \"number\",\n                        Description: \"first operand\",\n                    },\n                    \"b\": {\n                        Type:        \"number\",\n                        Description: \"second operand\",\n                    },\n                },\n                Required: []string{\"a\", \"b\"},\n            },\n        },\n    },\n}\n\nmessages := []llm.Message{\n    {Role: \"user\", Content: `add 2 and 40`},\n}\n\nquery := llm.Query{\n    Model:    model,\n    Messages: messages,\n    Tools:    toolsList,\n    Options:  options,\n}\n\nanswer, err := completion.Chat(modelRunnerURL, query, provider.DockerModelRunner)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21 completion bis:\", err)\n}\n\n// Search tool to call in the answer\ntool, err := answer.Message.ToolCalls.Find(\"addNumbers\")\nif err != nil {\n    log.Fatal(\"\ud83d\ude21 ToolCalls.Find bis:\", err)\n}\nresult, _ := tool.Function.ToJSONString()\nfmt.Println(result)\n</code></pre> <p>Note</p> <p>You can find an example in examples/80-tools-model-runner</p>"},{"location":"docker-model-runner-support/#create-embeddings","title":"Create embeddings","text":"<pre><code>// Create an embedding from a content\nembedding, err := embeddings.CreateEmbedding(\n    openAIURL,\n    llm.Query4Embedding{\n        Model:  \"ai/mxbai-embed-large\",\n        Prompt: \"thi is the content of the document\",               \n    },\n    \"unique identifier\",\n    provider.DockerModelRunner,\n)\n</code></pre> <p>Note</p> <p>You can find an example in examples/79-embeddings-memory-model-runner</p>"},{"location":"embeddings/","title":"\ud83d\udd16 Embeddings","text":""},{"location":"embeddings/#embeddings","title":"Embeddings","text":"<p>\ud83d\udce6 <code>embeddings</code> package</p>"},{"location":"embeddings/#create-embeddings","title":"Create embeddings","text":"<pre><code>embedding, err := embeddings.CreateEmbedding(\n    ollamaUrl,\n    llm.Query4Embedding{\n        Model:  \"all-minilm\",\n        Prompt: \"Jean-Luc Picard is a fictional character in the Star Trek franchise.\",\n    },\n    \"Picard\", // identifier\n)\n</code></pre>"},{"location":"embeddings/#vector-stores","title":"Vector stores","text":"<p>A vector store allows to store and search for embeddings in an efficient way.</p>"},{"location":"embeddings/#in-memory-vector-store","title":"In memory vector store","text":"<p>Create a store: <pre><code>store := embeddings.MemoryVectorStore{\n    Records: make(map[string]llm.VectorRecord),\n}\n</code></pre></p> <p>Save embeddings: <pre><code>store.Save(embedding)\n</code></pre></p> <p>Search embeddings: <pre><code>embeddingFromQuestion, err := embeddings.CreateEmbedding(\n    ollamaUrl,\n    llm.Query4Embedding{\n        Model:  \"all-minilm\",\n        Prompt: \"Who is Jean-Luc Picard?\",\n    },\n    \"question\",\n)\n// find the nearest vector\nsimilarity, _ := store.SearchMaxSimilarity(embeddingFromQuestion)\n\ndocumentsContent := `&lt;context&gt;&lt;doc&gt;` + similarity.Prompt + `&lt;/doc&gt;&lt;/context&gt;`\n</code></pre></p> <p>\ud83d\udc40 you will find a complete example in <code>examples/08-embeddings</code></p>"},{"location":"embeddings/#bbolt-vector-store","title":"Bbolt vector store","text":"<p>Bbolt is an embedded key/value database for Go.</p> <p>Create a store, and open an existing store: <pre><code>store := embeddings.BboltVectorStore{}\nstore.Initialize(\"../embeddings.db\")\n</code></pre></p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/09-embeddings-bbolt</li> <li>examples/09-embeddings-bbolt/create-embeddings: create and populate the vector store</li> <li>examples/09-embeddings-bbolt/use-embeddings: search similarities in the vector store</li> </ul>"},{"location":"embeddings/#redis-vector-store","title":"Redis vector store","text":"<p>Create a store, and open an existing store: <pre><code>redisStore := embeddings.RedisVectorStore{}\nerr := redisStore.Initialize(\"localhost:6379\", \"\", \"chronicles-bucket\")\n\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21:\", err)\n}\n</code></pre></p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/32-rag-with-redis</li> <li>examples/32-rag-with-redis/create-embeddings: create and populate the vector store</li> <li>examples/32-rag-with-redis/use-embeddings: search similarities in the vector store</li> </ul>"},{"location":"embeddings/#elasticsearch-vector-store","title":"Elasticsearch vector store","text":"<p>Create a store, and open an existing store: <pre><code>cert, _ := os.ReadFile(os.Getenv(\"ELASTIC_CERT_PATH\"))\n\nelasticStore := embeddings.ElasticSearchStore{}\nerr := elasticStore.Initialize(\n    []string{\n        os.Getenv(\"ELASTIC_ADDRESS\"),\n    },\n    os.Getenv(\"ELASTIC_USERNAME\"),\n    os.Getenv(\"ELASTIC_PASSWORD\"),\n    cert,\n    \"chronicles-index\",\n)\n</code></pre></p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/33-rag-with-elastic</li> <li>examples/33-rag-with-elastic/create-embeddings: create and populate the vector store</li> <li>examples/33-rag-with-elastic/use-embeddings: search similarities in the vector store</li> </ul>"},{"location":"embeddings/#daphnia-vector-store","title":"Daphnia vector store","text":"<p>Daphnia is another one of my projects to create an embedded vector database (useful to experiment).</p> <pre><code>// Initialize the vector store\nvectorStore := embeddings.DaphniaVectoreStore{}\nvectorStore.Initialize(\"my-data.gob\")\n</code></pre> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/65-hyde</li> <li>examples/74-rag-with-daphnia</li> </ul>"},{"location":"embeddings/#additional-data","title":"Additional data","text":"<p>you can add additional data to a vector record (embedding):</p> <pre><code>embedding.Text()\nembedding.Reference()\nembedding.MetaData()\n</code></pre>"},{"location":"embeddings/#create-embeddings-from-text-files-and-similarity-search","title":"Create embeddings from text files and Similarity search","text":""},{"location":"embeddings/#create-embeddings_1","title":"Create embeddings","text":"<pre><code>ollamaUrl := \"http://localhost:11434\"\nembeddingsModel := \"all-minilm\"\n\nstore := embeddings.BboltVectorStore{}\nstore.Initialize(\"../embeddings.db\")\n\n// Parse all golang source code of the examples\n// Create embeddings from documents and save them in the store\ncounter := 0\n_, err := content.ForEachFile(\"../../examples\", \".go\", func(path string) error {\n    data, err := os.ReadFile(path)\n    if err != nil {\n        return err\n    }\n\n    fmt.Println(\"\ud83d\udcdd Creating embedding from:\", path)\n    counter++\n    embedding, err := embeddings.CreateEmbedding(\n        ollamaUrl,\n        llm.Query4Embedding{\n            Model:  embeddingsModel,\n            Prompt: string(data),\n        },\n        strconv.Itoa(counter), // don't forget the id (unique identifier)\n    )\n    fmt.Println(\"\ud83d\udce6 Created: \", len(embedding.Embedding))\n\n    if err != nil {\n        fmt.Println(\"\ud83d\ude21:\", err)\n    } else {\n        _, err := store.Save(embedding)\n        if err != nil {\n            fmt.Println(\"\ud83d\ude21:\", err)\n        }\n    }\n    return nil\n})\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21:\", err)\n}\n</code></pre>"},{"location":"embeddings/#similarity-search","title":"Similarity search","text":"<pre><code>ollamaUrl := \"http://localhost:11434\"\nembeddingsModel := \"all-minilm\"\nchatModel := \"magicoder:latest\"\n\nstore := embeddings.BboltVectorStore{}\nstore.Initialize(\"../embeddings.db\")\n\nsystemContent := `You are a Golang developer and an expert in computer programming.\nPlease make friendly answer for the noobs. Use the provided context and doc to answer.\nAdd source code examples if you can.`\n\n// Question for the Chat system\nuserContent := `How to create a stream chat completion with Parakeet?`\n\n// Create an embedding from the user question\nembeddingFromQuestion, err := embeddings.CreateEmbedding(\n    ollamaUrl,\n    llm.Query4Embedding{\n        Model:  embeddingsModel,\n        Prompt: userContent,\n    },\n    \"question\",\n)\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21:\", err)\n}\nfmt.Println(\"\ud83d\udd0e searching for similarity...\")\n\nsimilarities, _ := store.SearchSimilarities(embeddingFromQuestion, 0.3)\n\n// Generate the context from the similarities\n// This will generate a string with a content like this one:\n// `&lt;context&gt;&lt;doc&gt;...&lt;doc&gt;&lt;doc&gt;...&lt;doc&gt;&lt;/context&gt;`\ndocumentsContent := embeddings.GenerateContextFromSimilarities(similarities)\n\nfmt.Println(\"\ud83c\udf89 similarities\", len(similarities))\n\noptions := llm.SetOptions(map[string]interface{}{\n    option.Temperature: 0.4,\n    option.RepeatLastN: 2,\n})\n\nquery := llm.Query{\n    Model: chatModel,\n    Messages: []llm.Message{\n        {Role: \"system\", Content: systemContent},\n        {Role: \"system\", Content: documentsContent},\n        {Role: \"user\", Content: userContent},\n    },\n    Options: options,\n    Stream: false,\n}\n\nfmt.Println(\"\")\nfmt.Println(\"\ud83e\udd16 answer:\")\n\n// Answer the question\n_, err = completion.ChatStream(ollamaUrl, query,\n    func(answer llm.Answer) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n</code></pre>"},{"location":"embeddings/#other-similarity-search-methods","title":"Other similarity search methods","text":"<p><code>SearchMaxSimilarity</code> searches for the vector record in the <code>BboltVectorStore</code> that has the maximum cosine distance similarity to the given <code>embeddingFromQuestion</code>: <pre><code>similarity, _ := store.SearchMaxSimilarity(embeddingFromQuestion)\n</code></pre></p> <p><code>SearchTopNSimilarities</code> searches for vector records in the <code>BboltVectorStore</code> that have a cosine distance similarity greater than or equal to the given <code>limit</code> and returns the top <code>n</code> records: <pre><code>similarities, _ := store.SearchTopNSimilarities(embeddingFromQuestion, limit, n)\n</code></pre></p>"},{"location":"error-handling/","title":"Error handling","text":"<p>\ud83d\udea7 work in progress</p>"},{"location":"error-handling/#modelnotfounderror","title":"ModelNotFoundError","text":"<pre><code>// package completion\ntype ModelNotFoundError struct {\n  Code    int\n  Message string\n  Model   string\n}\n</code></pre> <p>Usage: <pre><code>answer, err := completion.Chat(ollamaUrl, query)\nif err != nil {\n  // test if the model is not found\n  if modelErr, ok := err.(*completion.ModelNotFoundError); ok {\n    fmt.Printf(\"\ud83d\udca5 Got Model Not Found error: %s\\n\", modelErr.Message)\n    fmt.Printf(\"\ud83d\ude21 Error code: %d\\n\", modelErr.Code)\n    fmt.Printf(\"\ud83e\udde0 Expected Model: %s\\n\", modelErr.Model)\n  } else {\n    log.Fatal(\"\ud83d\ude21:\", err)\n  }\n}\n</code></pre></p>"},{"location":"error-handling/#nosuchollamahosterror","title":"NoSuchOllamaHostError","text":"<pre><code>// package completion\ntype NoSuchOllamaHostError struct {\n    Host string\n    Message string\n}\n</code></pre> <p>Usage: <pre><code>if noHostErr, ok := err.(*completion.NoSuchOllamaHostError); ok {\n  fmt.Printf(\"\ud83e\udd99 Got No Such Ollama Host error: %s\\n\", noHostErr.Message)\n  fmt.Printf(\"\ud83c\udf0d Expected Host: %s\\n\", noHostErr.Host)\n}\n</code></pre></p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/04-chat-stream</li> <li>examples/66-structured-outputs</li> </ul>"},{"location":"flock-agents/","title":"Flock Agents","text":"<p>Flock is a Parakeet package for creating and managing AI agents using the Ollama backend. It provides a simple way to create conversational agents, orchestrate interactions between them, and implement function calling capabilities.</p>"},{"location":"flock-agents/#basic-concepts","title":"Basic Concepts","text":""},{"location":"flock-agents/#agent","title":"Agent","text":"<p>An Agent represents an AI entity with specific configurations, instructions, and capabilities. Each agent can have: - Name - Model (Ollama model to use) - Instructions (static or dynamic) - Functions (for tool calling) - Options (model parameters)</p>"},{"location":"flock-agents/#orchestrator","title":"Orchestrator","text":"<p>The Orchestrator manages agent execution and interactions. It provides methods to: - Run single agent interactions - Stream responses - Execute function calls</p>"},{"location":"flock-agents/#creating-agents","title":"Creating Agents","text":""},{"location":"flock-agents/#basic-agent","title":"Basic Agent","text":"<pre><code>agent := flock.Agent{\n    Name: \"Bob\",\n    Model: \"qwen2.5:3b\",\n    OllamaUrl: \"http://localhost:11434\",\n    Options: llm.SetOptions(map[string]interface{}{\n        option.Temperature: 0.7,\n        option.TopK: 40,\n        option.TopP: 0.9,\n    }),\n}\n\n// Setting static instructions\nagent.SetInstructions(\"Help the user with their queries.\")\n\n// Setting dynamic instructions with context\nagent.SetInstructions(func(contextVars map[string]interface{}) string {\n    userName := contextVars[\"userName\"].(string)\n    return fmt.Sprintf(\"Help %s with their queries.\", userName)\n})\n</code></pre>"},{"location":"flock-agents/#using-the-orchestrator","title":"Using the Orchestrator","text":""},{"location":"flock-agents/#basic-interaction","title":"Basic Interaction","text":"<pre><code>orchestrator := flock.Orchestrator{}\n\nresponse, err := orchestrator.Run(\n    agent,\n    []llm.Message{\n        {Role: \"user\", Content: \"Hello, what's the best pizza?\"},\n    },\n    map[string]interface{}{\"userName\": \"Sam\"},\n)\n\nif err != nil {\n    log.Fatal(err)\n}\n\nfmt.Println(response.Messages[len(response.Messages)-1].Content)\n\n// Or:\nfmt.Println(response.GetLastMessage().Content)\n</code></pre>"},{"location":"flock-agents/#streaming-responses","title":"Streaming Responses","text":"<pre><code>response, err := orchestrator.RunStream(\n    agent,\n    messages,\n    contextVars,\n    func(answer llm.Answer) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    },\n)\n</code></pre>"},{"location":"flock-agents/#function-calling","title":"Function Calling","text":"<pre><code>options := llm.SetOptions(map[string]interface{}{\n    option.Temperature: 0.0,\n})\n\n// Create agent with function capability\ncalculator := flock.Agent{\n    Name:    \"Calculator\",\n    Model:   \"allenporter/xlam:1b\",\n    OllamaUrl: \"http://localhost:11434\",\n    Options: options,\n    Functions: map[string]flock.AgentFunction{\n        \"multiply\": func(args interface{}) (interface{}, error) {\n            argsMap := args.(map[string]interface{})\n            return argsMap[\"a\"].(float64) * argsMap[\"b\"].(float64), nil\n        },\n    },\n}\n\n// Define tool\ntools := []llm.Tool{\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"multiply\",\n            Description: \"Make a multiplication of the two given numbers\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"a\": {\n                        Type:        \"number\",\n                        Description: \"first operand\",\n                    },\n                    \"b\": {\n                        Type:        \"number\",\n                        Description: \"second operand\",\n                    },\n                },\n                Required: []string{\"a\", \"b\"},\n            },\n        },\n    },\n}\n\n// Run calculation\norchestrator := flock.Orchestrator{}\n\nresponse, _ := orchestrator.RunWithTools(\n    calculator,\n    []llm.Message{{Role: \"user\", Content: \"multiply 5 and 3\"}},\n    map[string]interface{}{},\n    tools,\n    true, // execute the tool(s)    \n)\n\n// Access result\nfmt.Println(\"Result:\", response.GetLastMessage().ToolCalls[0].Result)\n</code></pre>"},{"location":"flock-agents/#examples","title":"Examples","text":""},{"location":"flock-agents/#basic-agent_1","title":"Basic Agent","text":"<p>Note</p> <p>Look at this sample:</p> <ul> <li>examples/61-agents</li> </ul>"},{"location":"flock-agents/#conversational-agents","title":"Conversational Agents","text":"<p>Note</p> <p>Look at these samples:</p> <ul> <li>examples/62-agents-chat</li> <li>examples/63-agents-chat-stream</li> </ul>"},{"location":"flock-agents/#function-calling-example","title":"Function Calling Example","text":"<p>Note</p> <p>Look at this sample:</p> <ul> <li>examples/64-agents-with-tools</li> </ul>"},{"location":"function-calling-before-tools-support/","title":"Function Calling (before tools support)","text":""},{"location":"function-calling-before-tools-support/#function-calling-before-tool-support","title":"Function Calling (before tool support)","text":"<p>almost depecrated</p> <p>What is \"Function Calling\"? First, it's not a feature where a LLM can call and execute a function. \"Function Calling\" is the ability for certain LLMs to provide a specific output with the same format (we could say: \"a predictable output format\").</p> <p>So, the principle is simple:</p> <ul> <li>You (or your GenAI application) will create a prompt with a delimited list of tools (the functions) composed by name, descriptions, and parameters: <code>SayHello</code>, <code>AddNumbers</code>, etc.</li> <li>Then, you will add your question (\"Hey, say 'hello' to Bob!\") to the prompt and send all of this to the LLM.</li> <li>If the LLM \"understand\" that the <code>SayHello</code> function can be used to say \"hello\" to Bob, then the LLM will answer with only the name of the function with the parameter(s). For example: <code>{\"name\":\"SayHello\",\"arguments\":{\"name\":\"Bob\"}}</code>.</li> </ul> <p>Then, it will be up to you to implement the call of the function.</p> <p>The latest version (v0.3) of Mistral 7b supports function calling and is available for Ollama.</p>"},{"location":"function-calling-before-tools-support/#define-a-list-of-tools","title":"Define a list of tools","text":"<p>First, you have to provide the LLM with a list of tools with the following format:</p> <pre><code>toolsList := []llm.Tool{\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"hello\",\n            Description: \"Say hello to a given person with his name\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"name\": {\n                        Type:        \"string\",\n                        Description: \"The name of the person\",\n                    },\n                },\n                Required: []string{\"name\"},\n            },\n        },\n    },\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"addNumbers\",\n            Description: \"Make an addition of the two given numbers\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"a\": {\n                        Type:        \"number\",\n                        Description: \"first operand\",\n                    },\n                    \"b\": {\n                        Type:        \"number\",\n                        Description: \"second operand\",\n                    },\n                },\n                Required: []string{\"a\", \"b\"},\n            },\n        },\n    },\n}\n</code></pre>"},{"location":"function-calling-before-tools-support/#generate-a-prompt-from-the-tools-list-and-the-user-instructions","title":"Generate a prompt from the tools list and the user instructions","text":"<p>The <code>tools.GenerateContent</code> method generates a string with the tools in JSON format surrounded by <code>[AVAILABLE_TOOLS]</code> and <code>[/AVAILABLE_TOOLS]</code>: <pre><code>toolsContent, err := tools.GenerateContent(toolsList)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n</code></pre></p> <p>The <code>tools.GenerateInstructions</code> method generates a string with the user instructions surrounded by <code>[INST]</code> and <code>[/INST]</code>: <pre><code>userContent := tools.GenerateInstructions(`say \"hello\" to Bob`)\n</code></pre></p> <p>Then, you can add these two strings to the messages list: <pre><code>messages := []llm.Message{\n    {Role: \"system\", Content: toolsContent},\n    {Role: \"user\", Content: userContent},\n}\n</code></pre></p>"},{"location":"function-calling-before-tools-support/#send-the-prompt-messages-to-the-llm","title":"Send the prompt (messages) to the LLM","text":"<p>It's important to set the <code>Temperature</code> to <code>0.0</code>: <pre><code>options := llm.Options{\n    Temperature:   0.0,\n    RepeatLastN:   2,\n    RepeatPenalty: 2.0,\n}\n\n//You must set the `Format` to `json` and `Raw` to `true`:\nquery := llm.Query{\n    Model: model,\n    Messages: messages,\n    Options: options,\n    Format:  \"json\",\n    Raw:     true,\n}\n</code></pre></p> <p>When building the payload to be sent to Ollama, we need to set the <code>Raw</code> field to true, thanks to that, no formatting will be applied to the prompt (we override the prompt template of Mistral), and we need to set the <code>Format</code> field to <code>\"json\"</code>.</p> <p>No you can call the <code>Chat</code> method. The answer of the LLM will be in JSON format: <pre><code>answer, err := completion.Chat(ollamaUrl, query)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n// PrettyString is a helper that prettyfies the JSON string\nresult, err := gear.PrettyString(answer.Message.Content)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\nfmt.Println(result)\n</code></pre></p> <p>You should get this answer: <pre><code>{\n  \"name\": \"hello\",\n  \"arguments\": {\n    \"name\": \"Bob\"\n  }\n}\n</code></pre></p> <p>You can try with the other tool (or function): <pre><code>userContent := tools.GenerateInstructions(`add 2 and 40`)\n</code></pre></p> <p>You should get this answer: <pre><code>{\n  \"name\": \"addNumbers\",\n  \"arguments\": {\n    \"a\": 2,\n    \"b\": 40\n  }\n}\n</code></pre></p> <p>Remark: always test the format of the output, even if Mistral is trained for \"function calling\", the result are not entirely predictable.</p> <p>Note</p> <p>Look at this sample for a complete sample: examples/15-mistral-function-calling</p>"},{"location":"function-calling-without-tools-support/","title":"Function Calling (without tools support)","text":""},{"location":"function-calling-without-tools-support/#function-calling-with-llms-that-do-not-implement-tools-support","title":"Function Calling with LLMs that do not implement tools support","text":"<p>It is possible to reproduce this feature with some LLMs that do not implement the \"Function Calling\" feature natively, but we need to supervise them and explain precisely what we need. The result (the output) will be less predictable, so you will need to add some tests before using the output, but with some \"clever\" LLMs, you will obtain correct results. I did my experiments with phi3:mini.</p> <p>The trick is simple:</p> <p>Add this message at the begining of the list of messages: <pre><code>systemContentIntroduction := `You have access to the following tools:`\n</code></pre></p> <p>Add this message at the end of the list of messages, just before the user message: <pre><code>systemContentInstructions := `If the question of the user matched the description of a tool, the tool will be called.\nTo call a tool, respond with a JSON object with the following structure: \n{\n    \"name\": &lt;name of the called tool&gt;,\n    \"arguments\": {\n    &lt;name of the argument&gt;: &lt;value of the argument&gt;\n    }\n}\n\nsearch the name of the tool in the list of tools with the Name field\n`\n</code></pre></p> <p>At the end, you will have this: <pre><code>messages := []llm.Message{\n    {Role: \"system\", Content: systemContentIntroduction},\n    {Role: \"system\", Content: toolsContent},\n    {Role: \"system\", Content: systemContentInstructions},\n    {Role: \"user\", Content: `say \"hello\" to Bob`},\n}\n</code></pre></p> <p>Note</p> <p>Look at this sample for a complete sample: examples/17-fake-function-calling</p>"},{"location":"generate-completion/","title":"\ud83e\udd16 Generate completion","text":""},{"location":"generate-completion/#generate-completion","title":"Generate completion","text":""},{"location":"generate-completion/#completion","title":"Completion","text":"<p>The simple completion can be used to generate a response for a given prompt with a provided model.</p> <pre><code>package main\n\nimport (\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n    \"github.com/parakeet-nest/parakeet/enums/option\"\n\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"tinydolphin\"\n\n    options := llm.SetOptions(map[string]interface{}{\n        option.Temperature: 0.5,\n    })\n\n    question := llm.GenQuery{\n        Model: model,\n        Prompt: \"Who is James T Kirk?\",\n        Options: options,\n    }\n\n    answer, err := completion.Generate(ollamaUrl, question)\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n    fmt.Println(answer.Response)\n}\n</code></pre>"},{"location":"generate-completion/#completion-with-stream","title":"Completion with stream","text":"<pre><code>package main\n\nimport (\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"tinydolphin\"\n\n    options := llm.Options{\n        Temperature: 0.5,\n    }\n\n    question := llm.GenQuery{\n        Model: model,\n        Prompt: \"Who is James T Kirk?\",\n        Options: options,\n    }\n\n    answer, err := completion.GenerateStream(ollamaUrl, question,\n        func(answer llm.Answer) error {\n            fmt.Print(answer.Response)\n            return nil\n        })\n\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n}\n</code></pre>"},{"location":"generate-completion/#completion-with-context","title":"Completion with context","text":"<p>see: https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion</p> <p>The context can be used to keep a short conversational memory for the next completion.</p> <pre><code>package main\n\nimport (\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"tinydolphin\"\n\n    options := llm.Options{\n        Temperature: 0.5,\n    }\n\n    firstQuestion := llm.GenQuery{\n        Model: model,\n        Prompt: \"Who is James T Kirk?\",\n        Options: options,\n    }\n\n    answer, err := completion.Generate(ollamaUrl, firstQuestion)\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n    fmt.Println(answer.Response)\n\n    fmt.Println()\n\n    secondQuestion := llm.GenQuery{\n        Model: model,\n        Prompt: \"Who is his best friend?\",\n        Context: answer.Context,\n        Options: options,\n    }\n\n    answer, err = completion.Generate(ollamaUrl, secondQuestion)\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n    fmt.Println(answer.Response)\n}\n</code></pre>"},{"location":"llms-helpers/","title":"LLM helpers and Parakeet methods","text":""},{"location":"llms-helpers/#get-information-about-a-model","title":"Get Information about a model","text":"<pre><code>llm.ShowModelInformation(url, model string) (llm.ModelInformation, int, error)\n</code></pre> <p><code>ShowModelInformation</code> retrieves information about a model from the specified URL.</p> <p>Parameters:</p> <ul> <li><code>url</code>: the base URL of the API.</li> <li><code>model</code>: the name of the model to retrieve information for.</li> </ul> <p>Returns:</p> <ul> <li><code>ModelInformation</code>: the information about the model.</li> <li><code>int</code>: the HTTP status code of the response.</li> <li><code>error</code>: an error if the request fails.</li> </ul> <p>\u270b Remark: if the model does not exist, it will return an error with a status code of 404.</p> <p>If you use a protected Ollama endpoint, use this function:</p> <pre><code>llm.ShowModelInformationWithToken(url, model , tokenHeaderName, tokenHeaderValue string) (llm.ModelInformation, int, error)\n</code></pre>"},{"location":"llms-helpers/#pull-a-model","title":"Pull a model","text":"<pre><code>llm.PullModel(url, model string) (llm.PullResult, int, error)\n</code></pre> <p><code>PullModel</code> sends a POST request to the specified URL to pull a model with the given name.</p> <p>Parameters:</p> <ul> <li><code>url</code>: The URL to send the request to.</li> <li><code>model</code>: The name of the model to pull.</li> </ul> <p>Returns:</p> <ul> <li><code>PullResult</code>: The result of the pull operation.</li> <li><code>int</code>: The HTTP status code of the response.</li> <li><code>error</code>: An error if the request fails.</li> </ul> <p>If you use a protected Ollama endpoint, use this function:</p> <pre><code>llm.PullModelWithToken(url, model , tokenHeaderName, tokenHeaderValue string) (llm.PullResult, int, error)\n</code></pre>"},{"location":"llms-helpers/#get-the-list-of-the-installed-models","title":"Get the list of the installed models","text":"<pre><code>llm.GetModelsList(url string) (llm.ModelList, int, error)\n</code></pre> <p><code>GetModelsList</code> sends a GET request to the specified URL to fetch the list of the installed models.</p> <p>Parameters:</p> <ul> <li><code>url</code>: The URL to send the request to.</li> </ul> <p>Returns:</p> <ul> <li><code>ModelList</code>: The result of the reques, use the <code>models</code> property to get the list.</li> <li><code>int</code>: The HTTP status code of the response.</li> <li><code>error</code>: An error if the request fails.</li> </ul> <p>If you use a protected Ollama endpoint, use this function:</p> <pre><code>llm.GetModelsList(url , tokenHeaderName, tokenHeaderValue string) (llm.ModelList, int, error)\n</code></pre> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/50-llm-information</li> </ul>"},{"location":"mcp/","title":"Model Context Protocol","text":"<p>\ud83d\udea7 work in progress</p> <p>Integration of <code>github.com/mark3labs/mcp-go/mcp</code> and <code>github.com/mark3labs/mcp-go/client</code></p>"},{"location":"mcp/#stdio-transport","title":"STDIO transport","text":""},{"location":"mcp/#overview","title":"Overview","text":"<p>MCP (Model Context Protocol) with STDIO transport allows Parakeet to interact with external tools and services over standard input/output. This is particularly useful for integrating lightweight tools in a portable manner, leveraging command-line processes for execution.  </p>"},{"location":"mcp/#creating-an-mcp-stdio-client","title":"Creating an MCP STDIO Client","text":""},{"location":"mcp/#initializing-the-client","title":"Initializing the Client","text":"<p>The following example demonstrates how to initialize an MCP STDIO client:  </p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"time\"\n\n    \"github.com/parakeet-nest/parakeet/mcp-stdio\"\n)\n\nfunc main() {\n    // Create context with timeout\n    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n    defer cancel()\n\n    // Create MCP STDIO client\n    mcpClient, err := mcpstdio.NewClient(ctx, \"docker\", []string{}, \"run\", \"--rm\", \"-i\", \"mcp-curl\")\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n\n    // Initialize the client\n    _, err = mcpClient.Initialize()\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n\n    fmt.Println(\"\ud83d\ude80 MCP STDIO client initialized successfully.\")\n}\n</code></pre>"},{"location":"mcp/#listing-available-tools","title":"Listing Available Tools","text":"<p>To retrieve a list of tools available on the MCP STDIO server:  </p> <pre><code>tools, err := mcpClient.ListTools()\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21 Failed to list tools:\", err)\n}\n\nfmt.Println(\"\ud83d\udce6 Available Tools:\")\nfor _, tool := range tools {\n    fmt.Printf(\"- %s: %s\\n\", tool.Name, tool.Description)\n}\n</code></pre>"},{"location":"mcp/#executing-a-tool","title":"Executing a Tool","text":"<p>This example demonstrates how to call a tool that fetches webpage content:  </p> <pre><code>messages := []llm.Message{\n    {Role: \"user\", Content: \"Fetch this page: https://raw.githubusercontent.com/parakeet-nest/parakeet/main/README.md\"},\n}\n\noptions := llm.SetOptions(map[string]interface{}{\n    option.Temperature:   0.0,\n    option.RepeatLastN:   2,\n    option.RepeatPenalty: 2.0,\n})\n\nquery := llm.Query{\n    Model:    \"qwen2.5:0.5b\",\n    Messages: messages,\n    Tools:    tools,\n    Options:  options,\n}\n\nanswer, err := completion.Chat(\"http://localhost:11434\", query)\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\n// Extract the first tool call\ntoolCall := answer.Message.ToolCalls[0]\n\nfmt.Println(\"\ud83d\udee0\ufe0f Calling:\", toolCall.Function.Name, toolCall.Function.Arguments)\n\n// Execute tool\nmcpResult, err := mcpClient.CallTool(toolCall.Function.Name, toolCall.Function.Arguments)\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\nfmt.Println(\"\ud83c\udf0d Content:\", mcpResult.Text)\n</code></pre>"},{"location":"mcp/#handling-errors","title":"Handling Errors","text":"<p>MCP STDIO has various error types for debugging:  </p> <ul> <li><code>STDIOClientCreationError</code> </li> <li><code>STDIOClientInitializationError</code> </li> <li><code>STDIOGetToolsError</code> </li> <li><code>STDIOToolCallError</code> </li> <li><code>STDIOResultExtractionError</code> </li> <li><code>STDIOListResourcesError</code> </li> <li><code>STDIOReadResourceError</code> </li> </ul> <p>For example, error handling for initialization:  </p> <pre><code>_, err = mcpClient.Initialize()\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21 STDIO Initialization Failed:\", err)\n}\n</code></pre>"},{"location":"mcp/#running-mcp-stdio-with-docker","title":"Running MCP STDIO with Docker","text":"<p>If the MCP server is containerized, you can configure it to run inside a Docker container:  </p> <pre><code>docker build -t mcp-curl .\n</code></pre> <p>To start it:  </p> <pre><code>docker run --rm -i mcp-curl\n</code></pre> <p>Alternatively, using a JSON configuration file:  </p> <pre><code>{\n  \"mcpServers\": {\n    \"mcp-curl-with-docker\" :{\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"mcp-curl\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"mcp/#conclusion","title":"Conclusion","text":"<p>MCP STDIO provides a lightweight and flexible transport mechanism for integrating tools within Parakeet. This is ideal for scenarios where external commands need to be executed without requiring a persistent network server.</p>"},{"location":"mcp/#sse-transport","title":"SSE transport","text":""},{"location":"mcp/#overview_1","title":"Overview","text":"<p>MCP (Model Context Protocol) with SSE (Server-Sent Events) allows Parakeet to interact with an event-driven architecture for fetching resources, executing tools, and processing prompts dynamically. This integration facilitates seamless communication with LLM-based tools while leveraging event streams for efficiency.  </p>"},{"location":"mcp/#using-mcp-sse-client-with-parakeet","title":"Using MCP SSE Client with Parakeet","text":""},{"location":"mcp/#initializing-the-client_1","title":"Initializing the Client","text":"<pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"time\"\n\n    \"github.com/joho/godotenv\"\n    mcpsse \"github.com/parakeet-nest/parakeet/mcp-sse\"\n    \"github.com/parakeet-nest/parakeet/gear\"\n)\n\nfunc main() {\n    err := godotenv.Load()\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n\n    mcpSSEServerUrl := gear.GetEnvString(\"MCP_HOST\", \"http://0.0.0.0:5001\")\n\n    // Create context with timeout\n    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n    defer cancel()\n\n    // Create MCP client\n    mcpClient, err := mcpsse.NewClient(ctx, mcpSSEServerUrl)\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n\n    err = mcpClient.Start()\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n\n    result, err := mcpClient.Initialize()\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n\n    fmt.Println(\"\ud83d\ude80 Initialized with server:\", result.ServerInfo.Name, result.ServerInfo.Version)\n\n    mcpClient.Close()\n}\n</code></pre>"},{"location":"mcp/#executing-mcp-tools","title":"Executing MCP Tools","text":""},{"location":"mcp/#fetching-web-page-content","title":"Fetching Web Page Content","text":"<p>This example demonstrates how to use the <code>fetch</code> tool to retrieve webpage content:  </p> <pre><code>ollamaTools, err := mcpClient.ListTools()\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\nmessages := []llm.Message{\n    {Role: \"user\", Content: \"Fetch this page: https://raw.githubusercontent.com/parakeet-nest/parakeet/main/README.md\"},\n}\n\noptions := llm.SetOptions(map[string]interface{}{\n    option.Temperature:   0.0,\n    option.RepeatLastN:   2,\n    option.RepeatPenalty: 2.0,\n})\n\ntoolsQuery := llm.Query{\n    Model:    modelWithToolsSupport,\n    Messages: messages,\n    Tools:    ollamaTools,\n    Options:  options,\n}\n\nanswer, err := completion.Chat(ollamaUrl, toolsQuery)\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\ntoolCall := answer.Message.ToolCalls[0]\n\nfmt.Println(\"\ud83e\udd99\ud83d\udee0\ufe0f \ud83d\udce3 calling:\", toolCall.Function.Name, toolCall.Function.Arguments)\n\nmcpResult, err := mcpClient.CallTool(toolCall.Function.Name, toolCall.Function.Arguments)\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\nfmt.Println(\"\ud83c\udf0d Content:\", mcpResult.Text)\n</code></pre>"},{"location":"mcp/#getting-resources","title":"Getting Resources","text":"<p>The following example lists all available resources from the MCP SSE server:  </p> <pre><code>resources, err := mcpClient.ListResources()\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\nfmt.Println(\"\ud83d\udce6 Available Resources:\")\nfor _, resource := range resources {\n    fmt.Printf(\"- Name: %s, URI: %s, MIME Type: %s\\n\", resource.Name, resource.URI, resource.MIMEType)\n}\n</code></pre>"},{"location":"mcp/#handling-errors_1","title":"Handling Errors","text":"<p>MCP SSE has various error types for debugging:  </p> <ul> <li><code>SSEClientCreationError</code> </li> <li><code>SSEClientStartError</code> </li> <li><code>SSEClientInitializationError</code> </li> <li><code>SSEGetToolsError</code> </li> <li><code>SSEToolCallError</code> </li> <li><code>SSEListResourcesError</code> </li> <li><code>SSEReadResourceError</code> </li> </ul> <p>For example, error handling for initialization:  </p> <pre><code>result, err := mcpClient.Initialize()\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21 SSE Initialization Failed:\", err)\n}\n</code></pre>"},{"location":"mcp/#conclusion_1","title":"Conclusion","text":"<p>MCP SSE provides a structured way to interact with streaming data and tools in an LLM-powered environment using Parakeet. By leveraging this integration, developers can seamlessly manage event-driven AI workflows.</p>"},{"location":"mcp/#http-transport","title":"HTTP transport","text":""},{"location":"mcp/#overview_2","title":"Overview","text":"<p>MCP (Model Context Protocol) with HTTP transport allows Parakeet to interact with MCP servers over standard HTTP connections. This transport method provides a lightweight, stateless approach to tool execution and resource management, making it suitable for serverless architectures and web-based integrations.</p>"},{"location":"mcp/#using-mcp-http-client-with-parakeet","title":"Using MCP HTTP Client with Parakeet","text":""},{"location":"mcp/#initializing-the-client_2","title":"Initializing the Client","text":"<pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"time\"\n\n    \"github.com/joho/godotenv\"\n    mcphttp \"github.com/parakeet-nest/parakeet/mcp-http\"\n    \"github.com/parakeet-nest/parakeet/gear\"\n)\n\nfunc main() {\n    err := godotenv.Load()\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n\n    mcpHTTPServerUrl := gear.GetEnvString(\"MCP_HOST\", \"http://localhost:9090\")\n\n    // Create context with timeout\n    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n    defer cancel()\n\n    // Create MCP HTTP client\n    mcpClient, err := mcphttp.NewClient(ctx, mcpHTTPServerUrl)\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n\n    err = mcpClient.Start()\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n\n    result, err := mcpClient.Initialize()\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n\n    fmt.Println(\"\ud83d\ude80 Initialized with server:\", result.ServerInfo.Name, result.ServerInfo.Version)\n\n    defer mcpClient.Close()\n}\n</code></pre>"},{"location":"mcp/#executing-mcp-tools_1","title":"Executing MCP Tools","text":""},{"location":"mcp/#rolling-dice-example","title":"Rolling Dice Example","text":"<p>This example demonstrates how to use the <code>rool_dices</code> tool available on the HTTP server:  </p> <pre><code>ollamaTools, err := mcpClient.ListTools()\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\nfmt.Println(\"\ud83d\udee0\ufe0f Tools found:\")\nfor _, tool := range ollamaTools {\n    fmt.Println(\"  -\", tool.Function.Name, \":\", tool.Function.Description)\n}\n\nmessages := []llm.Message{\n    {Role: \"user\", Content: \"Can you roll 3 dices with 6 sides?\"},\n}\n\noptions := llm.SetOptions(map[string]any{\n    option.Temperature:   0.2,\n    option.RepeatLastN:   2,\n    option.RepeatPenalty: 2.0,\n})\n\ntoolsQuery := llm.Query{\n    Model:    modelWithToolsSupport,\n    Messages: messages,\n    Tools:    ollamaTools,\n    Options:  options,\n}\n\nchatCompletion, err := completion.Chat(ollamaUrl, toolsQuery)\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\nfmt.Println(\"\ud83d\udfe2 \ud83e\udd99 Answer:\", chatCompletion.Message.Content)\n\nif len(chatCompletion.Message.ToolCalls) &gt; 0 {\n    fmt.Println(\"\ud83d\udfe2 \ud83e\udd99 Tool Calls:\", chatCompletion.Message.ToolCalls)\n\n    toolCall := chatCompletion.Message.ToolCalls[0]\n    fmt.Println(\"\ud83d\udd27 Calling tool:\", toolCall.Function.Name)\n    fmt.Println(\"  Arguments:\", toolCall.Function.Arguments)\n\n    result, err := mcpClient.CallTool(toolCall.Function.Name, toolCall.Function.Arguments)\n    if err != nil {\n        log.Fatalln(\"\ud83d\ude21\", err)\n    }\n    fmt.Println(\"  Result:\", result.Text)\n}\n</code></pre>"},{"location":"mcp/#getting-resources_1","title":"Getting Resources","text":"<p>The HTTP client supports resource management similar to other transports:  </p> <pre><code>resources, err := mcpClient.ListResources()\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\nfmt.Println(\"\ud83d\udce6 Available Resources:\")\nfor _, resource := range resources {\n    fmt.Printf(\"- Name: %s, URI: %s, MIME Type: %s\\n\", resource.Name, resource.URI, resource.MIMEType)\n}\n\n// Read a specific resource\nresourceResult, err := mcpClient.ReadResource(\"file://example.txt\")\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\nfmt.Println(\"\ud83d\udcc4 Resource Contents:\", resourceResult.Contents)\n</code></pre>"},{"location":"mcp/#working-with-prompts","title":"Working with Prompts","text":"<p>List and execute prompts from the HTTP server:  </p> <pre><code>prompts, err := mcpClient.ListPrompts()\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\nfmt.Println(\"\ud83d\udcdd Available Prompts:\")\nfor _, prompt := range prompts {\n    fmt.Printf(\"- %s: %s\\n\", prompt.Name, prompt.Description)\n}\n\n// Get and fill a prompt\npromptResult, err := mcpClient.GetAndFillPrompt(\"example-prompt\", map[string]string{\n    \"arg1\": \"value1\",\n    \"arg2\": \"value2\",\n})\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21\", err)\n}\n\nfmt.Println(\"\ud83c\udfaf Prompt Result:\", promptResult)\n</code></pre>"},{"location":"mcp/#handling-errors_2","title":"Handling Errors","text":"<p>MCP HTTP has various error types for debugging:  </p> <ul> <li><code>HTTPClientCreationError</code> </li> <li><code>HTTPClientStartError</code> </li> <li><code>HTTPClientInitializationError</code> </li> <li><code>HTTPGetToolsError</code> </li> <li><code>HTTPToolCallError</code> </li> <li><code>HTTPResultExtractionError</code> </li> <li><code>HTTPListResourcesError</code> </li> <li><code>HTTPReadResourceError</code> </li> <li><code>HTTPListPromptsError</code> </li> <li><code>HTTPGetPromptError</code> </li> </ul> <p>For example, error handling for initialization:  </p> <pre><code>result, err := mcpClient.Initialize()\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21 HTTP Initialization Failed:\", err)\n}\n</code></pre>"},{"location":"mcp/#running-an-mcp-http-server","title":"Running an MCP HTTP Server","text":"<p>Here's an example of running the included HTTP server:  </p> <pre><code>cd examples/95-mcp-http/mcp-server\ngo run main.go\n</code></pre> <p>The server will start on port 9090 by default and provide a <code>/mcp</code> endpoint for the client to connect to.</p>"},{"location":"mcp/#bearer-token-authentication","title":"Bearer Token Authentication","text":"<p>The HTTP client supports bearer token authentication (implementation pending):  </p> <pre><code>// Bearer token will be supported in future versions\nmcpClient, err := mcphttp.NewClient(ctx, mcpHTTPServerUrl, \"your-bearer-token\")\n</code></pre>"},{"location":"mcp/#conclusion_2","title":"Conclusion","text":"<p>MCP HTTP provides a stateless, web-friendly transport mechanism for integrating tools within Parakeet. This is ideal for scenarios where you need HTTP-based tool execution, serverless deployments, or simple web service integrations.</p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/67-mcp</li> <li>examples/75-mcp-sse</li> <li>examples/95-mcp-http</li> </ul>"},{"location":"openaiapi-support/","title":"\ud83c\udf00 OpenAI API support","text":""},{"location":"openaiapi-support/#openai-api-support","title":"OpenAI API support","text":"<p>\u270b Only tested with the <code>gpt-4o-mini</code> model, and <code>text-embedding-3-large</code> for the embeddings model.</p> <p>Since the release <code>0.2.7</code> I unified the completion methods</p>"},{"location":"openaiapi-support/#chat-completion","title":"Chat completion","text":"<p>Use: <code>completion.Chat(openAIURL, query, provider.OpenAI, &lt;OPENAI-API-KEY&gt;)</code></p> <pre><code>openAIURL := \"https://api.openai.com/v1\"\nmodel := \"gpt-4o-mini\"\n\nsystemContent := `You are an expert in Star Trek.`\nuserContent := `Who is Jean-Luc Picard?`\n\nquery := llm.Query{\n    Model: model,\n    Messages: []llm.Message{\n        {Role: \"system\", Content: systemContent},\n        {Role: \"user\", Content: userContent},\n    },\n}\n\nanswer, err := completion.Chat(openAIURL, query, provider.OpenAI, os.Getenv(\"OPENAI_API_KEY\"))\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\nfmt.Println(answer.Message.Content)\n</code></pre>"},{"location":"openaiapi-support/#chat-completion-with-stream","title":"Chat completion with stream","text":"<p>Use: <code>completion.ChatStream(openAIURL, query, function, provider.OpenAI, &lt;OPENAI-API-KEY&gt;)</code></p> <pre><code>openAIURL := \"https://api.openai.com/v1\"\nmodel := \"gpt-4o-mini\"\n\nsystemContent := `You are an expert in Star Trek.`\nuserContent := `Who is Jean-Luc Picard?`\n\nquery := llm.Query{\n    Model: model,\n    Messages: []llm.Message{\n        {Role: \"system\", Content: systemContent},\n        {Role: \"user\", Content: userContent},\n    },\n}\n\n_, err = completion.ChatStream(openAIURL, query,\n    func(answer llm.Answer) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    }, provider.OpenAI, os.Getenv(\"OPENAI_API_KEY\"))\n\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n</code></pre> <p>Note</p> <p>You can find examples in </p> <ul> <li>examples/44-chat-openai</li> <li>examples/45-chat-stream-openai</li> </ul>"},{"location":"openaiapi-support/#chat-completion-with-tools","title":"Chat completion with tools","text":"<p>Use: <code>completion.Chat(openAIURL, query, provider.OpenAI, &lt;OPENAI-API-KEY&gt;)</code> and set <code>query.Tools</code>.</p> <pre><code>openAIURL := \"https://api.openai.com/v1\"\nmodel := \"gpt-4o-mini\"\nopenAIKey := os.Getenv(\"OPENAI_API_KEY\")\n\ntoolsList := []llm.Tool{\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"addNumbers\",\n            Description: \"Make an addition of the two given numbers\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"a\": {\n                        Type:        \"number\",\n                        Description: \"first operand\",\n                    },\n                    \"b\": {\n                        Type:        \"number\",\n                        Description: \"second operand\",\n                    },\n                },\n                Required: []string{\"a\", \"b\"},\n            },\n        },\n    },\n}\n\noptions := llm.SetOptions(map[string]interface{}{\n    option.Temperature: 0.0,\n})\n\nmessages := []llm.Message{\n    {Role: \"user\", Content: `add 2 and 40`},\n}\n\nquery := llm.Query{\n    Model:    model,\n    Messages: messages,\n    Tools:    toolsList,\n    Options:  options,\n}\n\nanswer, err := completion.Chat(openAIURL, query, provider.OpenAI, openAIKey)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n\n// Search tool to call in the answer\ntool, err := answer.Message.ToolCalls.Find(\"addNumbers\")\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\nresult, _ := tool.Function.ToJSONString()\nfmt.Println(result)\n</code></pre> <p>Note</p> <p>You can find an example in examples/81-tools-openai</p>"},{"location":"openaiapi-support/#create-embeddings","title":"Create embeddings","text":"<pre><code>// Create an embedding from a content\nembedding, err := embeddings.CreateEmbedding(\n    openAIURL,\n    llm.Query4Embedding{\n        Model:  \"text-embedding-3-large\",\n        Prompt: \"thi is the content of the document\",               \n    },\n    \"unique identifier\",\n    provider.OpenAI,\n    os.Getenv(\"OPENAI_API_KEY\"),\n)\n</code></pre> <p>Note</p> <p>You can find an example in examples/49-embeddings-memory-openai</p>"},{"location":"parakeet-blog/","title":"Blog Post","text":"<p>Note</p> <p>This page need to be updated with the latest blog posts.</p> <p>Info</p> <p>This is a collection of Parakeet blog posts showcasing the ease of creating GenAI applications with Ollama, Golang, and other tools.</p> <ul> <li>Parakeet, an easy way to create GenAI applications with Ollama and Golang</li> <li>Understand RAG with Parakeet -Function Calling with Ollama, Mistral 7B, Bash and Jq</li> <li>Function Calling with Ollama and LLMs that do not support function calling</li> </ul>"},{"location":"parakeet-demos/","title":"\ud83d\ude80 Parakeet Demos","text":""},{"location":"parakeet-demos/#parakeet-demos","title":"Parakeet Demos","text":"<p>Note</p> <p>These demos need to be updated</p> <p>Info</p> <p>This is a collection of Parakeet demos showcasing the ease of creating GenAI applications with Ollama, Golang, and other tools.</p> <ul> <li>https://github.com/parakeet-nest/parakeet-demo</li> <li>https://github.com/parakeet-nest/tiny-genai-stack</li> </ul>"},{"location":"parakeet-examples/","title":"Parakeet examples &amp; Recipes","text":"<p>\ud83d\udea7 work in progress</p> <p>From this repository: https://github.com/parakeet-nest/parakeet/tree/main/examples</p>"},{"location":"parsing-markdown/","title":"Parsing Markdown","text":"<p>!!! info \"\ud83d\udea7 work in progress\" (TODO: add more examples)</p> <p>I created some helpers to help to split markdown documents and create better chunks.</p> <ul> <li><code>ParseMarkdownWithHierarchy</code> chunks a markdown document while maintaining semantic meaning and preserving the relationship between sections.</li> </ul> <pre><code>chunks := content.ParseMarkdownWithHierarchy(document)\n</code></pre> <p><code>func ParseMarkdownWithHierarchy(document string) []Chunk</code></p> <p>You will get the following data: <pre><code>chunk := Chunk{\n    Level:        level,\n    Prefix:       prefix,\n    Header:       header,\n    Content:      strings.TrimSpace(content),\n    ParentPrefix: parent.Prefix,\n    ParentLevel:  parent.Level,\n    ParentHeader: parent.Header,\n}\n</code></pre> Then you can add meta data when creating the vectors thanks to these fields: <code>ParentPrefix</code>, <code>ParentLevel</code>, <code>ParentHeader</code>.</p> <ul> <li><code>ParseMarkdownWithLineage</code> parses the given markdown content and returns a slice of Chunk structs. Each Chunk represents a header and its associated content, along with its hierarchical lineage.</li> </ul> <pre><code>chunks := content.ParseMarkdownWithLineage(document)\n</code></pre> <p><code>func ParseMarkdownWithLineage(document string) []Chunk</code></p> <p>You will get the following data: <pre><code>chunk := Chunk{\n    Level:        level,\n    Prefix:       prefix,\n    Header:       header,\n    Content:      strings.TrimSpace(content),\n    ParentPrefix: parent.Prefix,\n    ParentLevel:  parent.Level,\n    ParentHeader: parent.Header,\n    Lineage:      lineage,\n}\n</code></pre> Then you can add meta data when creating the vectors thanks to this field: <code>Lineage</code>.</p> <p><code>Lineage</code> will keep the path of the sections. For example, with this document:</p> <pre><code># Tiefling Species in Fantasy Realms: A Comprehensive Analysis\n\n... some text ...\n\n## Professional Development and Education\n\n... some text ...\n</code></pre> <p>The <code>Lineage</code> value of the chunk of the second section will be:</p> <pre><code>Tiefling Species in Fantasy Realms: A Comprehensive Analysis &gt; Professional Development and Education\n</code></pre> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/65-hyde</li> </ul>"},{"location":"parsing-source-code/","title":"Parsing Source Code","text":"<p>\ud83d\udea7 work in progress</p>"},{"location":"parsing-source-code/#extract-elements-from-source-code","title":"Extract elements from source code:","text":"<ul> <li><code>source.ExtractCodeElements(fileContent string, language string) ([]CodeElement, error)</code></li> </ul> <pre><code>// CodeElement represents a code structure element (class, function, method)\ntype CodeElement struct {\n    Type        string // \"class\", \"function\", \"method\"\n    Name        string\n    Signature   string\n    Description string\n    LineNumber  int\n    ParentClass string // For methods\n    Parameters  []string\n    Source      string // Source code of the element\n}\n</code></pre> <p>the <code>Signature</code> could be useful to add context to embeddings.</p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/72-gitingest-es</li> <li>examples/73-gitingest-daphnia</li> <li>examples/74-rag-with-daphnia</li> </ul>"},{"location":"prompt-helpers/","title":"\ud83d\udd28 Prompt Helpers","text":""},{"location":"prompt-helpers/#prompt-helpers","title":"Prompt helpers","text":""},{"location":"prompt-helpers/#meta-prompts","title":"Meta prompts","text":"<p>package: <code>prompt</code></p> <p>Meta-prompts are special instructions embedded within a prompt to guide a language model in generating a specific kind of response.</p> Meta-Prompt Purpose [Brief] What is AI? For a concise answer [In Layman\u2019s Terms] Explain LLM For a simplified explanation [As a Story] Describe the evolution of cars To get the information in story form [Pros and Cons] Is AI useful? For a balanced view with advantages and disadvantages [Step-by-Step] How to do a smart prompt? For a detailed, step-by-step guide [Factual] What is the best pizza of the world? For a straightforward, factual answer [Opinion] What is the best pizza of the world? To get an opinion-based answer [Comparison] Compare pineapple pizza to pepperoni pizza For a comparative analysis [Timeline] What are the key milestones to develop a WebApp? For a chronological account of key events [As a Poem] How to cook a cake? For a poetic description [For Kids] How to cook a cake? For a child-friendly explanation [Advantages Only] What are the benefits of AI? To get a list of only the advantages [As a Recipe] How to cook a cake? To receive the information in the form of a recipe"},{"location":"prompt-helpers/#meta-prompts-methods","title":"Meta prompts methods","text":"<ul> <li><code>prompt.Brief(s string) string</code></li> <li><code>prompt.InLaymansTerms(s string) string</code></li> <li><code>prompt.AsAStory(s string) string</code></li> <li><code>prompt.ProsAndCons(s string) string</code></li> <li><code>prompt.StepByStep(s string) string</code></li> <li><code>prompt.Factual(s string) string</code></li> <li><code>prompt.Opinion(s string) string</code></li> <li><code>prompt.Comparison(s string) string</code></li> <li><code>prompt.Timeline(s string) string</code></li> <li><code>prompt.AsAPoem(s string) string</code></li> <li><code>prompt.ForKids(s string) string</code></li> <li><code>prompt.AdvantagesOnly(s string) string</code></li> <li><code>prompt.AsARecipe(s string) string</code></li> </ul>"},{"location":"protected-endpoint/","title":"Protected endpoint","text":"<p>If your Ollama endpoint is protected with a header token, you can specify the token like this:</p> <pre><code>query := llm.Query{\n    Model: model,\n    Messages: []llm.Message{\n        {Role: \"system\", Content: systemContent},\n        {Role: \"user\", Content: userContent},\n    },\n    Options: options,\n    TokenHeaderName: \"X-TOKEN\",\n    TokenHeaderValue: \"john_doe\",\n}\n</code></pre>"},{"location":"set-options/","title":"How to set the Options of the query","text":"<p>The best way to set the options of the query is to start from the default Ollama options and override the fields you want to change.</p> <pre><code>options := llm.DefaultOptions()\n// override the default value\noptions.Temperature = 0.5\n</code></pre> <p>Or you can use the <code>SetOptions</code> helper. This helper will set the default values for the fields not defined in the map:</p> <p>Define only the fields you want to override: <pre><code>options := llm.SetOptions(map[string]interface{}{\n  \"Temperature\": 0.5,\n})\n</code></pre></p> <p>Or use the <code>SetOptions</code> helper with the <code>option</code> enums: <pre><code>options := llm.SetOptions(map[string]interface{}{\n  option.Temperature: 0.5,\n  option.RepeatLastN: 2,\n})\n</code></pre></p> <p>Note</p> <p>Before, the JSON serialization of the <code>Options</code> used the <code>omitempty</code> tag.</p> <p>The <code>omitempty</code> tag prevents a field from being serialised if its value is the zero value for the field's type (e.g., 0.0 for float64).</p> <p>That means when <code>Temperature</code> equals <code>0.0</code>, the field is not serialised (then Ollama will use the <code>Temperature</code> default value, which equals <code>0.8</code>).</p> <p>The problem will happen for every value equal to <code>0</code> or <code>0.0</code></p> <p>Since now, the <code>omitempty</code> tag is removed from the <code>Options</code> struct.</p>"},{"location":"squawk-doc/","title":"Squawk Documentation","text":"<p>\ud83d\udea7 work in progress</p> <p>\"Squawk is the jQuery of generative AI\"</p> <p>Squawk simplifies common tasks with generative AI, making the technology more accessible and easier to work with.</p>"},{"location":"squawk-doc/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Installation</li> <li>Core Concepts</li> <li>Basic Methods</li> <li>Message Management</li> <li>Chat Completions</li> <li>Streaming Responses</li> <li>Structured Output</li> <li>Embeddings and RAG</li> <li>Function Calling with Tools</li> <li>Meta Prompts</li> <li>Complete Examples</li> </ul>"},{"location":"squawk-doc/#overview","title":"Overview","text":"<p>Squawk provides a simplified interface for interacting with generative AI models and embeddings. It encapsulates various configurations, states, and tools required for processing and generating responses from language models. It supports functionalities such as setting models, managing conversation contexts, handling embeddings, and executing structured or streaming outputs.</p> <p>Key features: - Simplified interaction with language models and embeddings - Support for multiple providers (OpenAI, Ollama, Docker Model Runner) - Flexible configuration options for chat and structured outputs - Tools for managing conversation contexts and embeddings - Callback-based execution for chat and streaming responses</p>"},{"location":"squawk-doc/#installation","title":"Installation","text":"<p>To use Squawk in your Go project:</p> <pre><code>go get github.com/parakeet-nest/parakeet/squawk\n</code></pre>"},{"location":"squawk-doc/#core-concepts","title":"Core Concepts","text":""},{"location":"squawk-doc/#squawk-structure","title":"Squawk Structure","text":"<p>The Squawk struct contains all the configurations, states, and tools required for interacting with language models:</p> <pre><code>type Squawk struct {\n    setOfMessages   []llm.Message\n    baseUrl         string\n    apiUrl          string\n    provider        string\n    chatModel       string\n    embeddingsModel string\n    options         llm.Options\n    openAPIKey      string\n    lastAnswer      llm.Answer\n    lastError       error\n    schema          map[string]any\n\n    // embeddings\n    vectorStore  embeddings.VectorStore\n    similarities []llm.VectorRecord\n\n    // tools\n    tools     []llm.Tool\n    toolCalls []llm.ToolCall\n}\n</code></pre>"},{"location":"squawk-doc/#creating-a-new-squawk-instance","title":"Creating a New Squawk Instance","text":"<p>Start by creating a new Squawk instance:</p> <pre><code>sq := squawk.New()\n</code></pre>"},{"location":"squawk-doc/#method-chaining","title":"Method Chaining","text":"<p>Squawk follows the method chaining pattern, allowing for a fluent interface:</p> <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    System(\"You are a Go expert\").\n    User(\"Explain channels in Go\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        fmt.Println(answer.Message.Content)\n    })\n</code></pre>"},{"location":"squawk-doc/#basic-methods","title":"Basic Methods","text":""},{"location":"squawk-doc/#model","title":"Model","text":"<p>Sets the chat model identifier for the Squawk instance.</p> <pre><code>func (s *Squawk) Model(model string) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama)\n</code></pre></p>"},{"location":"squawk-doc/#embeddingsmodel","title":"EmbeddingsModel","text":"<p>Sets the embeddings model identifier for the Squawk instance.</p> <pre><code>func (s *Squawk) EmbeddingsModel(model string) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    EmbeddingsModel(\"nomic-embed-text\").\n    Provider(provider.Ollama)\n</code></pre></p>"},{"location":"squawk-doc/#baseurl","title":"BaseURL","text":"<p>Sets the base URL for the API endpoint of the chosen provider.</p> <pre><code>func (s *Squawk) BaseURL(url string) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    BaseURL(\"http://localhost:11434\").\n    Provider(provider.Ollama)\n</code></pre></p>"},{"location":"squawk-doc/#provider","title":"Provider","text":"<p>Sets the LLM provider for the Squawk instance and configures the API URL and other parameters.</p> <pre><code>func (s *Squawk) Provider(llmProvider string, parameters ...string) *Squawk\n</code></pre> <p>Examples:</p> <pre><code>// Ollama\nsquawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama)\n\n// OpenAI\nsquawk.New().\n    Model(\"gpt-4\").\n    Provider(provider.OpenAI, \"your-api-key\")\n\n// Docker Model Runner\nsquawk.New().\n    Model(\"llama:13b\").\n    Provider(provider.DockerModelRunner)\n</code></pre>"},{"location":"squawk-doc/#options","title":"Options","text":"<p>Sets the configuration options for the language model interactions.</p> <pre><code>func (s *Squawk) Options(options llm.Options) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    Options(llm.SetOptions(map[string]interface{}{\n        option.Temperature:   0.7,\n        option.TopP:          0.9,\n        option.RepeatLastN:   64,\n        option.RepeatPenalty: 1.1,\n    }))\n</code></pre></p> <p>Common options: - Temperature: Controls randomness (0.0 to 1.0) - TopP: Nucleus sampling parameter - RepeatLastN: Number of tokens to look back for repetitions - RepeatPenalty: Penalty for repeated tokens - TopK: Limits vocabulary to top K tokens - MaxTokens: Maximum tokens to generate</p>"},{"location":"squawk-doc/#message-management","title":"Message Management","text":""},{"location":"squawk-doc/#system","title":"System","text":"<p>Adds a system message to the conversation context.</p> <pre><code>func (s *Squawk) System(message string, optionalParameters ...string) *Squawk\n</code></pre> <p>Examples: <pre><code>// Basic system message\nsquawk.New().\n    System(\"You are a helpful AI assistant specializing in Go programming\")\n\n// Labeled system message\nsquawk.New().\n    System(\"You are a code review expert\", \"code-reviewer\")\n</code></pre></p>"},{"location":"squawk-doc/#user","title":"User","text":"<p>Adds a user message to the conversation context.</p> <pre><code>func (s *Squawk) User(message string, optionalParameters ...string) *Squawk\n</code></pre> <p>Examples: <pre><code>// Basic user message\nsquawk.New().\n    User(\"What is the capital of France?\")\n\n// Labeled user message\nsquawk.New().\n    User(\"Review this Go code...\", \"code-review-1\")\n</code></pre></p>"},{"location":"squawk-doc/#assistant","title":"Assistant","text":"<p>Adds an assistant message to the conversation context.</p> <pre><code>func (s *Squawk) Assistant(message string, optionalParameters ...string) *Squawk\n</code></pre> <p>Examples: <pre><code>// Basic assistant message\nsquawk.New().\n    Assistant(\"A goroutine is a lightweight thread of execution in Go\")\n\n// Labeled assistant message\nsquawk.New().\n    Assistant(\"The function looks good but needs error handling\", \"review-1\")\n</code></pre></p>"},{"location":"squawk-doc/#addsetofmessages","title":"AddSetOfMessages","text":"<p>Creates or extends a conversation with provided messages.</p> <pre><code>func (s *Squawk) AddSetOfMessages(messages ...interface{}) *Squawk\n</code></pre> <p>Examples: <pre><code>// Adding single messages\nsquawk.New().\n    AddSetOfMessages(\n        llm.Message{Role: \"system\", Content: \"You are a Go expert\"},\n        llm.Message{Role: \"user\", Content: \"Explain channels\"},\n    )\n\n// Adding a slice of messages\npreviousMessages := []llm.Message{\n    {Role: \"system\", Content: \"You are a Go expert\"},\n    {Role: \"user\", Content: \"What is concurrency?\"},\n    {Role: \"assistant\", Content: \"Concurrency is...\"},\n}\n\nsquawk.New().\n    AddSetOfMessages(previousMessages).\n    User(\"How does this relate to goroutines?\")\n</code></pre></p>"},{"location":"squawk-doc/#messages","title":"Messages","text":"<p>Returns the current conversation context as a slice of messages.</p> <pre><code>func (s *Squawk) Messages() []llm.Message\n</code></pre> <p>Example: <pre><code>messages := squawk.Messages()\nfor _, msg := range messages {\n    fmt.Printf(\"Role: %s, Content: %s\\n\", msg.Role, msg.Content)\n}\n</code></pre></p>"},{"location":"squawk-doc/#saveassistantanswer","title":"SaveAssistantAnswer","text":"<p>Adds the last model response to the conversation history as an assistant message.</p> <pre><code>func (s *Squawk) SaveAssistantAnswer(optionalParameters ...string) *Squawk\n</code></pre> <p>Examples: <pre><code>// Save without label\nsquawk.New().\n    Chat(/* ... */).\n    SaveAssistantAnswer()\n\n// Save with label\nsquawk.New().\n    Chat(/* ... */).\n    SaveAssistantAnswer(\"answer-1\")\n</code></pre></p>"},{"location":"squawk-doc/#lastanswer","title":"LastAnswer","text":"<p>Gets or sets the most recent answer from the language model.</p> <pre><code>func (s *Squawk) LastAnswer(optionalAnswer ...llm.Answer) llm.Answer\n</code></pre> <p>Example: <pre><code>// Get last answer\nlastAnswer := squawk.LastAnswer()\nfmt.Println(\"Last response:\", lastAnswer.Message.Content)\n\n// Set custom answer\ncustomAnswer := llm.Answer{\n    Message: llm.Message{\n        Role: \"assistant\",\n        Content: \"Custom response\",\n    },\n}\nsquawk.LastAnswer(customAnswer)\n</code></pre></p>"},{"location":"squawk-doc/#removemessagebylabel","title":"RemoveMessageByLabel","text":"<p>Removes messages with the specified label from the conversation context.</p> <pre><code>func (s *Squawk) RemoveMessageByLabel(label string) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    System(\"You are a Go expert\", \"role\").\n    User(\"What is a channel?\", \"question-1\").\n    Chat(/* ... */).\n    SaveAssistantAnswer(\"answer-1\").\n    RemoveMessageByLabel(\"question-1\")\n</code></pre></p>"},{"location":"squawk-doc/#chat-completions","title":"Chat Completions","text":""},{"location":"squawk-doc/#chat","title":"Chat","text":"<p>Executes a non-streaming chat completion request and handles the response through a callback function.</p> <pre><code>func (s *Squawk) Chat(callBack func(answer llm.Answer, self *Squawk, err error)) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    System(\"You are a Go expert\").\n    User(\"What is a channel?\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        if err != nil {\n            fmt.Printf(\"Error: %v\\n\", err)\n            return\n        }\n        fmt.Println(answer.Message.Content)\n    })\n</code></pre></p>"},{"location":"squawk-doc/#streaming-responses","title":"Streaming Responses","text":""},{"location":"squawk-doc/#chatstream","title":"ChatStream","text":"<p>Executes a streaming chat completion request and handles the response through a callback function.</p> <pre><code>func (s *Squawk) ChatStream(callBack func(answer llm.Answer, self *Squawk) error) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    System(\"You are a Go expert\").\n    User(\"Explain channels in detail\").\n    ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n        if answer.Error != nil {\n            fmt.Printf(\"Stream error: %v\\n\", answer.Error)\n            return answer.Error\n        }\n\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n</code></pre></p>"},{"location":"squawk-doc/#structured-output","title":"Structured Output","text":""},{"location":"squawk-doc/#schema","title":"Schema","text":"<p>Sets a schema for structured output from the language model.</p> <pre><code>func (s *Squawk) Schema(schema map[string]any) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    Schema(map[string]any{\n        \"type\": \"object\",\n        \"properties\": map[string]any{\n            \"name\": map[string]any{\n                \"type\": \"string\",\n                \"description\": \"The name of the person\",\n            },\n            \"age\": map[string]any{\n                \"type\": \"integer\",\n                \"description\": \"The age of the person\",\n            },\n        },\n    })\n</code></pre></p>"},{"location":"squawk-doc/#structuredoutput","title":"StructuredOutput","text":"<p>Processes a structured chat completion request and handles the response through a callback function.</p> <pre><code>func (s *Squawk) StructuredOutput(callBack func(answer llm.Answer, self *Squawk, err error)) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    Schema(map[string]any{\n        \"type\": \"object\",\n        \"properties\": map[string]any{\n            \"functionName\": map[string]any{\n                \"type\": \"string\",\n                \"description\": \"Name of the function\",\n            },\n            \"complexity\": map[string]any{\n                \"type\": \"string\",\n                \"enum\": [\"O(1)\", \"O(n)\", \"O(n^2)\"],\n                \"description\": \"Time complexity of the function\",\n            },\n        },\n    }).\n    User(\"Analyze this function: func BubbleSort(arr []int) []int { ... }\").\n    StructuredOutput(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        if err != nil {\n            fmt.Println(\"Error:\", err)\n            return\n        }\n        result := answer.StructuredOutput\n        fmt.Printf(\"Function: %s\\nComplexity: %s\\n\",\n            result[\"functionName\"],\n            result[\"complexity\"])\n    })\n</code></pre></p>"},{"location":"squawk-doc/#embeddings-and-rag","title":"Embeddings and RAG","text":""},{"location":"squawk-doc/#store","title":"Store","text":"<p>Sets the vector store for embeddings in the Squawk instance.</p> <pre><code>func (s *Squawk) Store(store embeddings.VectorStore, optionalParameters ...string) *Squawk\n</code></pre> <p>Example: <pre><code>memStore := embeddings.NewMemoryVectorStore()\nsquawk.New().\n    EmbeddingsModel(\"nomic-embed-text\").\n    Provider(provider.Ollama).\n    Store(memStore)\n</code></pre></p>"},{"location":"squawk-doc/#generateembeddings","title":"GenerateEmbeddings","text":"<p>Creates vector embeddings for a list of documents and stores them in the configured vector store.</p> <pre><code>func (s *Squawk) GenerateEmbeddings(docs []string, optionalParameters ...any) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    EmbeddingsModel(\"nomic-embed-text\").\n    Provider(provider.Ollama).\n    Store(embeddings.NewMemoryVectorStore()).\n    GenerateEmbeddings(\n        []string{\n            \"Go is a statically typed language\",\n            \"Go has built-in concurrency support\",\n        },\n        true  // Enable logging\n    )\n</code></pre></p>"},{"location":"squawk-doc/#similaritysearch","title":"SimilaritySearch","text":"<p>Performs a semantic search against stored embeddings to find similar documents.</p> <pre><code>func (s *Squawk) SimilaritySearch(content string, limit float64, max int, optionalParameters ...any) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    EmbeddingsModel(\"nomic-embed-text\").\n    Provider(provider.Ollama).\n    Store(embeddings.NewMemoryVectorStore()).\n    GenerateEmbeddings([]string{\n        \"Go is a statically typed language\",\n        \"Go supports concurrent programming\",\n    }).\n    SimilaritySearch(\"concurrent Go features\", 0.7, 3, true)\n</code></pre></p>"},{"location":"squawk-doc/#similaritysearchfromusermessage","title":"SimilaritySearchFromUserMessage","text":"<p>Performs a semantic search using the content of a labeled user message as the search query.</p> <pre><code>func (s *Squawk) SimilaritySearchFromUserMessage(userMessageLabel string, limit float64, max int, optionalParameters ...any) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    EmbeddingsModel(\"nomic-embed-text\").\n    Provider(provider.Ollama).\n    Store(embeddings.NewMemoryVectorStore()).\n    GenerateEmbeddings([]string{\n        \"Go is a statically typed language\",\n        \"Go supports concurrent programming\",\n    }).\n    User(\"Tell me about Go concurrency\", \"question-1\").\n    SimilaritySearchFromUserMessage(\"question-1\", 0.7, 3, true)\n</code></pre></p>"},{"location":"squawk-doc/#addsimilaritiestomessages","title":"AddSimilaritiesToMessages","text":"<p>Adds the context generated from similarity search results to the conversation as a system message.</p> <pre><code>func (s *Squawk) AddSimilaritiesToMessages(optionalParameters ...string) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    SimilaritySearch(\"concurrent Go features\", 0.7, 3).\n    AddSimilaritiesToMessages(\"context-1\").\n    User(\"Explain these concepts\")\n</code></pre></p>"},{"location":"squawk-doc/#addsimilaritiestomessageswithprefix","title":"AddSimilaritiesToMessagesWithPrefix","text":"<p>Adds the context generated from similarity search results to the conversation as a system message, with a custom prefix.</p> <pre><code>func (s *Squawk) AddSimilaritiesToMessagesWithPrefix(prefix string, optionalParameters ...string) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    SimilaritySearch(\"concurrent Go features\", 0.7, 3).\n    AddSimilaritiesToMessagesWithPrefix(\n        \"Use this documentation as context: \\n\\n\",\n        \"context-1\",\n    ).\n    User(\"Explain these concepts\")\n</code></pre></p>"},{"location":"squawk-doc/#similarities","title":"Similarities","text":"<p>Returns the vector records from the most recent similarity search.</p> <pre><code>func (s *Squawk) Similarities() []llm.VectorRecord\n</code></pre> <p>Example: <pre><code>similarities := squawk.Similarities()\nfor _, record := range similarities {\n    fmt.Printf(\"Score: %.2f, Content: %s\\n\", \n        record.Score, \n        record.Content,\n    )\n}\n</code></pre></p>"},{"location":"squawk-doc/#contextfromsimilarities","title":"ContextFromSimilarities","text":"<p>Generates a formatted string containing the content from similarity search results.</p> <pre><code>func (s *Squawk) ContextFromSimilarities() string\n</code></pre> <p>Example: <pre><code>context := squawk.ContextFromSimilarities()\nfmt.Println(\"Retrieved context:\", context)\n</code></pre></p>"},{"location":"squawk-doc/#function-calling-with-tools","title":"Function Calling with Tools","text":""},{"location":"squawk-doc/#tools","title":"Tools","text":"<p>Sets the list of available tools for function calling capabilities.</p> <pre><code>func (s *Squawk) Tools(toolsList []llm.Tool) *Squawk\n</code></pre> <p>Example: <pre><code>toolsList := []llm.Tool{\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"get_weather\",\n            Description: \"Get current weather for a location\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"location\": {\n                        Type:        \"string\",\n                        Description: \"City name\",\n                    },\n                },\n                Required: []string{\"location\"},\n            },\n        },\n    },\n}\n\nsquawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    Tools(toolsList)\n</code></pre></p>"},{"location":"squawk-doc/#functioncalling","title":"FunctionCalling","text":"<p>Executes a function calling request and processes the results through a callback function.</p> <pre><code>func (s *Squawk) FunctionCalling(callBack func(answer llm.Answer, self *Squawk, err error)) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    Tools(toolsList).\n    User(\"Find all records about Go programming\").\n    FunctionCalling(func(answer llm.Answer, s *squawk.Squawk, err error) {\n        if err != nil {\n            fmt.Println(\"Error:\", err)\n            return\n        }\n\n        for _, call := range s.ToolCalls() {\n            fmt.Printf(\"Function: %s\\nArguments: %v\\n\", \n                call.Name, \n                call.Arguments,\n            )\n        }\n    })\n</code></pre></p>"},{"location":"squawk-doc/#toolcalls","title":"ToolCalls","text":"<p>Returns the tool calls from the most recent function calling interaction.</p> <pre><code>func (s *Squawk) ToolCalls() []llm.ToolCall\n</code></pre> <p>Example: <pre><code>calls := s.ToolCalls()\nfor _, call := range calls {\n    fmt.Printf(\"Called: %s with %v\\n\", call.Name, call.Arguments)\n}\n</code></pre></p>"},{"location":"squawk-doc/#meta-prompts","title":"Meta Prompts","text":"<p>Squawk provides several methods that modify the input message to the LLM using predefined prompting templates.</p>"},{"location":"squawk-doc/#forkids","title":"ForKids","text":"<p>Formats a message to be kid-friendly.</p> <pre><code>func (s *Squawk) ForKids(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#brief","title":"Brief","text":"<p>Formats a message to get a brief response.</p> <pre><code>func (s *Squawk) Brief(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#asapoem","title":"AsAPoem","text":"<p>Formats a message to get a response in the form of a poem.</p> <pre><code>func (s *Squawk) AsAPoem(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#advantagesonly","title":"AdvantagesOnly","text":"<p>Formats a message to get only advantages in the response.</p> <pre><code>func (s *Squawk) AdvantagesOnly(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#asarecipe","title":"AsARecipe","text":"<p>Formats a message to get a response in the form of a recipe.</p> <pre><code>func (s *Squawk) AsARecipe(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#timeline","title":"Timeline","text":"<p>Formats a message to get a timeline in the response.</p> <pre><code>func (s *Squawk) Timeline(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#comparison","title":"Comparison","text":"<p>Formats a message to get a comparison in the response.</p> <pre><code>func (s *Squawk) Comparison(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#opinion","title":"Opinion","text":"<p>Formats a message to get an opinion in the response.</p> <pre><code>func (s *Squawk) Opinion(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#factual","title":"Factual","text":"<p>Formats a message to get factual information in the response.</p> <pre><code>func (s *Squawk) Factual(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#stepbystep","title":"StepByStep","text":"<p>Formats a message to get a step-by-step explanation in the response.</p> <pre><code>func (s *Squawk) StepByStep(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#prosandcons","title":"ProsAndCons","text":"<p>Formats a message to get pros and cons in the response.</p> <pre><code>func (s *Squawk) ProsAndCons(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#asastory","title":"AsAStory","text":"<p>Formats a message to get a response in the form of a story.</p> <pre><code>func (s *Squawk) AsAStory(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#inlaymansterms","title":"InLaymansTerms","text":"<p>Formats a message to get a response in simple terms.</p> <pre><code>func (s *Squawk) InLaymansTerms(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#summarize","title":"Summarize","text":"<p>Formats a message to get a summary in the response.</p> <pre><code>func (s *Squawk) Summarize(message string, optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#summarizelastanswer","title":"SummarizeLastAnswer","text":"<p>Formats a message to get a summary of the last answer in the response.</p> <pre><code>func (s *Squawk) SummarizeLastAnswer(optionalParameters ...string) *Squawk\n</code></pre>"},{"location":"squawk-doc/#error-handling","title":"Error Handling","text":""},{"location":"squawk-doc/#lasterror","title":"LastError","text":"<p>Gets or sets the most recent error encountered during processing.</p> <pre><code>func (s *Squawk) LastError(optionalError ...error) error\n</code></pre> <p>Example: <pre><code>// Check for errors\nif err := squawk.LastError(); err != nil {\n    fmt.Printf(\"Last operation failed: %v\\n\", err)\n}\n\n// Set an error\nsquawk.LastError(errors.New(\"custom error\"))\n</code></pre></p>"},{"location":"squawk-doc/#miscellaneous","title":"Miscellaneous","text":""},{"location":"squawk-doc/#cmd","title":"Cmd","text":"<p>Executes a custom command function on the Squawk instance.</p> <pre><code>func (s *Squawk) Cmd(callBack func(self *Squawk)) *Squawk\n</code></pre> <p>Example: <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    System(\"You are a Go expert\").\n    Cmd(func(self *squawk.Squawk) {\n        fmt.Printf(\"Current model: %s\\n\", self.chatModel)\n        fmt.Printf(\"Messages count: %d\\n\", len(self.Messages()))\n    }).\n    User(\"What is a channel?\")\n</code></pre></p>"},{"location":"squawk-doc/#complete-examples","title":"Complete Examples","text":""},{"location":"squawk-doc/#basic-chat-example","title":"Basic Chat Example","text":"<pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    BaseURL(\"http://localhost:11434\").\n    System(\"You are a Go expert\").\n    User(\"Explain concurrency in Go\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        if err != nil {\n            fmt.Println(\"Error:\", err)\n            return\n        }\n        fmt.Println(answer.Message.Content)\n    })\n</code></pre>"},{"location":"squawk-doc/#streaming-chat-example","title":"Streaming Chat Example","text":"<pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    System(\"You are a Go expert\").\n    User(\"Explain channels in Go\").\n    ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n        if answer.Error != nil {\n            fmt.Println(\"Error:\", answer.Error)\n            return answer.Error\n        }\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n</code></pre>"},{"location":"squawk-doc/#rag-example","title":"RAG Example","text":"<pre><code>// Sample documents\ndocs := []string{\n    \"Go is a statically typed language\",\n    \"Go supports concurrent programming\",\n}\n\n// Initialize vector store\nstore := embeddings.NewMemoryVectorStore()\n\nsquawk.New().\n    EmbeddingsModel(\"nomic-embed-text\").\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    Store(store).\n    GenerateEmbeddings(docs, true).\n    System(\"You are a Go expert\").\n    User(\"Tell me about Go concurrency\", \"question-1\").\n    SimilaritySearchFromUserMessage(\"question-1\", 0.7, 3).\n    AddSimilaritiesToMessages(\"context\").\n    ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n</code></pre>"},{"location":"squawk-doc/#structured-output-example","title":"Structured Output Example","text":"<pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    Schema(map[string]any{\n        \"type\": \"object\",\n        \"properties\": map[string]any{\n            \"name\": map[string]any{\n                \"type\": \"string\",\n            },\n            \"capital\": map[string]any{\n                \"type\": \"string\",\n            },\n            \"languages\": map[string]any{\n                \"type\": \"array\",\n                \"items\": map[string]any{\n                    \"type\": \"string\",\n                },\n            },\n        },\n        \"required\": []string{\"name\", \"capital\", \"languages\"},\n    }).\n    User(\"Tell me about Canada\").\n    StructuredOutput(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        if err != nil {\n            fmt.Println(\"Error:\", err)\n            return\n        }\n        fmt.Println(\"Structured Output:\", answer.StructuredOutput)\n    })\n</code></pre>"},{"location":"squawk-doc/#function-calling-example","title":"Function Calling Example","text":"<pre><code>toolsList := []llm.Tool{\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"hello\",\n            Description: \"Say hello to a given person with their name\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"name\": {\n                        Type:        \"string\",\n                        Description: \"The name of the person\",\n                    },\n                },\n                Required: []string{\"name\"},\n            },\n        },\n    },\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"addNumbers\",\n            Description: \"Make an addition of the two given numbers\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"a\": {\n                        Type:        \"number\",\n                        Description: \"first operand\",\n                    },\n                    \"b\": {\n                        Type:        \"number\",\n                        Description: \"second operand\",\n                    },\n                },\n                Required: []string{\"a\", \"b\"},\n            },\n        },\n    },\n}\n\nsquawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    Tools(toolsList).\n    User(`say \"hello\" to Bob, say \"hello\" to Sam`).\n    User(`add 2 and 40`).\n    FunctionCalling(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        var results string\n        for _, toolCall := range self.ToolCalls() {\n            switch toolCall.Function.Name {\n            case \"hello\":\n                results += fmt.Sprintf(\"Hello %s\\n\", toolCall.Function.Arguments[\"name\"])\n            case \"addNumbers\":\n                a := toolCall.Function.Arguments[\"a\"]\n                b := toolCall.Function.Arguments[\"b\"]\n                results += fmt.Sprintf(\"Addition of %v and %v is %v\\n\", \n                    a, b, a.(float64)+b.(float64))\n            }\n        }\n        self.System(\"RESULTS:\\n\"+results)\n    }).\n    User(\"Use the results and format the output with fancy emojis\").\n    ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n</code></pre>"},{"location":"squawk-doc/#meta-prompts-example","title":"Meta Prompts Example","text":"<pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    ForKids(\"Explain Docker\").\n    ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n\nsquawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    Brief(\"Explain Docker\").\n    ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n\nsquawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    InLaymansTerms(\"Explain Docker\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        fmt.Print(answer.Message.Content)\n    }).\n    SummarizeLastAnswer().\n    ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n</code></pre>"},{"location":"squawk-getting-started/","title":"Squawk: Getting Started","text":"<p>Info</p> <p>Squawk is like the \"jQuery\" of Generative AI</p> <p>Squawk is a DSL for Parakeet designed to simplify interactions with language models, making generative AI more accessible and easier to work with - similar to how jQuery simplified JavaScript development. </p>"},{"location":"squawk-getting-started/#small-chat-with-squawk","title":"Small chat with Squawk","text":"<pre><code>ollamaBaseUrl := \"http://localhost:11434\"\nmodel := \"qwen2.5:1.5b\"\n\noptions := llm.SetOptions(map[string]interface{}{\n    option.Temperature:   0.5,\n    option.RepeatLastN:   2,\n    option.RepeatPenalty: 2.2,\n})\n\nollamaParrot := squawk.New().\n    Model(model).\n    BaseURL(ollamaBaseUrl).\n    Provider(provider.Ollama)\n\nollamaParrot.\n    Options(options).\n    System(\"You are a useful AI agent, you are a Star Trek expert.\").\n    User(\"Who is James T Kirk?\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        fmt.Println(answer.Message.Content)\n    }).\n    SaveAssistantAnswer()\n\nollamaParrot.\n    User(\"Who is his best friend?\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        fmt.Println(answer.Message.Content)\n    })\n</code></pre>"},{"location":"squawk-getting-started/#you-can-chain-everything","title":"You can chain everything","text":"<pre><code>squawk.New().\n    Model(model).\n    BaseURL(ollamaBaseUrl).\n    Provider(provider.Ollama).\n    Options(options).\n    System(\"You are a useful AI agent, you are a Star Trek expert.\").\n    User(\"Who is James T Kirk?\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        fmt.Println(answer.Message.Content)\n    }).\n    SaveAssistantAnswer().\n    User(\"Who is his best friend?\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        fmt.Println(answer.Message.Content)\n    })\n</code></pre>"},{"location":"squawk-tutorial/","title":"Squawk Tutorial:","text":"<p>Note</p> <p>This tutorial covers the core functionality of Squawk. For more advanced use cases and detailed API documentation, refer to the official Squawk documentation and source code.</p> <p>This tutorial covers how to use Squawk for chat completions, retrieval-augmented generation (RAG), structured outputs, and function calling.</p>"},{"location":"squawk-tutorial/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Basic Chat Completions</li> <li>Retrieval-Augmented Generation (RAG)</li> <li>Structured Output</li> <li>Function Calling with Tools</li> <li>Additional Features</li> </ul>"},{"location":"squawk-tutorial/#basic-chat-completions","title":"Basic Chat Completions","text":""},{"location":"squawk-tutorial/#simple-chat-completion","title":"Simple Chat Completion","text":"<pre><code>squawk.New().\n  Model(\"qwen2.5:3b\").                    // Set the model\n  Provider(provider.Ollama).              // Set the provider\n  BaseURL(\"http://localhost:11434\").      // Set the API URL\n  System(\"You are a Go expert\").          // Add a system message\n  User(\"Explain concurrency in Go\").      // Add a user message\n  Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n    if err != nil {\n      fmt.Println(\"Error:\", err)\n      return\n    }\n    fmt.Println(answer.Message.Content)   // Print response\n  })\n</code></pre>"},{"location":"squawk-tutorial/#streaming-chat-completion","title":"Streaming Chat Completion","text":"<p>For real-time responses as they are generated:</p> <pre><code>squawk.New().\n  Model(\"qwen2.5:3b\").\n  Provider(provider.Ollama).\n  System(\"You are a Go expert\").\n  User(\"Explain channels in Go\").\n  ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n    fmt.Print(answer.Message.Content)     // Print each chunk\n    return nil\n  })\n</code></pre>"},{"location":"squawk-tutorial/#conversation-management","title":"Conversation Management","text":"<p>You can maintain a conversation by saving the assistant's responses:</p> <pre><code>sq := squawk.New().\n  Model(\"mistral:latest\").\n  Provider(provider.Ollama).\n  System(\"You are a Go expert\").\n  User(\"What is a goroutine?\").\n  Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n    if err != nil {\n      return\n    }\n    fmt.Println(answer.Message.Content)\n  }).\n  SaveAssistantAnswer()                   // Save response to conversation history\n\n// Continue the conversation\nsq.User(\"How do they differ from threads?\").\n  Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n    if err != nil {\n      return\n    }\n    fmt.Println(answer.Message.Content)\n  })\n</code></pre>"},{"location":"squawk-tutorial/#message-labeling-and-management","title":"Message Labeling and Management","text":"<p>Squawk allows labeling messages and selectively removing them:</p> <pre><code>squawk.New().\n  Model(\"qwen2.5:3b\").\n  Provider(provider.Ollama).\n  System(\"You are a Go expert\", \"role\").\n  User(\"What is a channel?\", \"question-1\").\n  Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n    // Handle response\n  }).\n  SaveAssistantAnswer(\"answer-1\").\n  RemoveMessageByLabel(\"question-1\").     // Remove specific message\n  User(\"How do goroutines work?\", \"question-2\")\n</code></pre>"},{"location":"squawk-tutorial/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"<p>RAG enhances responses by retrieving relevant information from a knowledge base.</p>"},{"location":"squawk-tutorial/#setting-up-a-vector-store","title":"Setting Up a Vector Store","text":"<pre><code>// Initialize a memory vector store\nstore := embeddings.MemoryVectorStore{\n    Records: make(map[string]llm.VectorRecord),\n}\n</code></pre>"},{"location":"squawk-tutorial/#generating-embeddings","title":"Generating Embeddings","text":"<pre><code>// Sample documents\ndocs := []string{\n  \"Go is a statically typed language\",\n  \"Go supports concurrent programming\",\n}\n\nsq := squawk.New().\n  EmbeddingsModel(\"mxbai-embed-large:latest\").     // Set embeddings model\n  Model(\"qwen2.5:3b\").\n  Provider(provider.Ollama).\n  BaseURL(\"http://localhost:11434\").\n  Store(&amp;store).                            // Set the vector store\n  GenerateEmbeddings(docs, true)           // Generate embeddings with logging\n</code></pre>"},{"location":"squawk-tutorial/#similarity-search","title":"Similarity Search","text":"<pre><code>// Search by content\nsq.SimilaritySearch(\n    \"concurrent programming\", // Query text\n    0.7,                      // Similarity threshold (0-1)\n    2,                        // Max results\n)\n\n// Get similarities\nsimilarities := sq.Similarities()\nfor _, sim := range similarities {\n    fmt.Println(\"Similar Content: \",sim.Prompt)\n}\n</code></pre>"},{"location":"squawk-tutorial/#search-from-user-message","title":"Search from User Message","text":"<pre><code>sq.User(\"Tell me about Go concurrency\", \"question-1\").\n  SimilaritySearchFromUserMessage(\"question-1\", 0.8, 3)\n</code></pre>"},{"location":"squawk-tutorial/#adding-context-to-conversation","title":"Adding Context to Conversation","text":"<pre><code>sq.SimilaritySearch(\"concurrent Go features\", 0.8, 3).\n  AddSimilaritiesToMessages().\n  User(\"Explain these concepts\").\n  Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n    fmt.Println(answer.Message.Content)\n  })\n\n// With custom prefix\nsq.SimilaritySearch(\"concurrent Go features\", 0.8, 3).\nAddSimilaritiesToMessagesWithPrefix(\"Use this documentation as context: \\n\\n\").    \nUser(\"Explain these concepts\").\nChat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n    fmt.Println(answer.Message.Content)\n})\n</code></pre>"},{"location":"squawk-tutorial/#complete-rag-example","title":"Complete RAG Example","text":"<pre><code>// Initialize a memory vector store\nstore := embeddings.MemoryVectorStore{\n    Records: make(map[string]llm.VectorRecord),\n}\n\n// Star Wars characters example\nstarWarsChars := []string{\n  `Luke Skywalker is the main protagonist of the original Star Wars trilogy...`,\n  `Princess Leia Organa is a leader of the Rebel Alliance...`,\n}\n\nsquawk.New().\n    EmbeddingsModel(\"mxbai-embed-large:latest\").\n    Model(\"qwen2.5:3b\").\n    Provider(provider.Ollama).\n    Store(&amp;store).\n    GenerateEmbeddings(starWarsChars, true).\n    System(\"You are a Star Wars expert\").\n    User(\"Who is Luke Skywalker?\", \"q1\").\n    SimilaritySearchFromUserMessage(\"q1\", 0.6, 1).\n    AddSimilaritiesToMessages(\"sim1\").\n    ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    }).\n    SaveAssistantAnswer(\"a1\").\n    RemoveMessageByLabel(\"q1\").\n    RemoveMessageByLabel(\"sim1\").\n    User(\"Who is Leia?\", \"q2\").\n    SimilaritySearchFromUserMessage(\"q2\", 0.6, 1).\n    AddSimilaritiesToMessages().\n    ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n</code></pre>"},{"location":"squawk-tutorial/#structured-output","title":"Structured Output","text":"<p>Structured output allows you to get responses in a specific JSON format.</p>"},{"location":"squawk-tutorial/#basic-structured-output","title":"Basic Structured Output","text":"<pre><code>squawk.New().\n    Model(\"qwen2.5:3b\").\n    Provider(provider.Ollama).\n    Schema(map[string]any{\n        \"type\": \"object\",\n        \"properties\": map[string]any{\n            \"name\": map[string]any{\n                \"type\":        \"string\",\n                \"description\": \"The name of the person\",\n            },\n            \"age\": map[string]any{\n                \"type\":        \"integer\",\n                \"description\": \"The age of the person\",\n            },\n        },\n        \"required\": []string{\"name\", \"age\"},\n    }).\n    User(\"Extract name and age from: John is 25 years old\").\n    StructuredOutput(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        if err != nil {\n            fmt.Println(\"Error:\", err)\n            return\n        }\n        fmt.Printf(\"%+v\\n\", answer.Message.Content)\n    })\n</code></pre>"},{"location":"squawk-tutorial/#complex-schema-example","title":"Complex Schema Example","text":"<pre><code>squawk.New().\n    Model(\"qwen2.5:3b\").\n    Provider(provider.Ollama).\n    Schema(map[string]any{\n        \"type\": \"object\",\n        \"properties\": map[string]any{\n            \"name\": map[string]any{\n                \"type\": \"string\",\n            },\n            \"capital\": map[string]any{\n                \"type\": \"string\",\n            },\n            \"languages\": map[string]any{\n                \"type\": \"array\",\n                \"items\": map[string]any{\n                    \"type\": \"string\",\n                },\n            },\n        },\n        \"required\": []string{\"name\", \"capital\", \"languages\"},\n    }).\n    User(\"Tell me about Canada\").\n    StructuredOutput(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        if err != nil {\n            fmt.Println(\"Error:\", err)\n            return\n        }\n        fmt.Println(\"Structured Output:\", answer.Message.Content)\n    })\n</code></pre>"},{"location":"squawk-tutorial/#function-calling-with-tools","title":"Function Calling with Tools","text":"<p>Function calling allows the model to call predefined functions or tools.</p>"},{"location":"squawk-tutorial/#defining-tools","title":"Defining Tools","text":"<pre><code>toolsList := []llm.Tool{\n  {\n    Type: \"function\",\n    Function: llm.Function{\n      Name:        \"get_weather\",\n      Description: \"Get current weather for a location\",\n      Parameters: llm.Parameters{\n        Type: \"object\",\n        Properties: map[string]llm.Property{\n          \"location\": {\n            Type:        \"string\",\n            Description: \"City name\",\n          },\n        },\n        Required: []string{\"location\"},\n      },\n    },\n  },\n  {\n    Type: \"function\",\n    Function: llm.Function{\n      Name:        \"calculate\",\n      Description: \"Perform a calculation\",\n      Parameters: llm.Parameters{\n        Type: \"object\",\n        Properties: map[string]llm.Property{\n          \"operation\": {\n            Type:        \"string\",\n            Description: \"Math operation\",\n          },\n          \"a\": {\n            Type:        \"number\",\n            Description: \"First number\",\n          },\n          \"b\": {\n            Type:        \"number\",\n            Description: \"Second number\",\n          },\n        },\n        Required: []string{\"operation\", \"a\", \"b\"},\n      },\n    },\n  },\n}\n</code></pre>"},{"location":"squawk-tutorial/#using-function-calling","title":"Using Function Calling","text":"<pre><code>squawk.New().\n    Model(\"qwen2.5:3b\").\n    Provider(provider.Ollama).\n    Tools(toolsList).\n    User(\"What's the weather in Paris and calculate 2 + 2\").\n    FunctionCalling(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        if err != nil {\n            fmt.Println(\"Error:\", err)\n            return\n        }\n\n        for _, toolCall := range self.ToolCalls() {\n            fmt.Printf(\"Tool: %s\\n\", toolCall.Function.Name)\n            fmt.Printf(\"Arguments: %v\\n\", toolCall.Function.Arguments)\n\n            // Implement function logic\n            switch toolCall.Function.Name {\n            case \"get_weather\":\n                location := toolCall.Function.Arguments[\"location\"]\n                // Call weather API\n                fmt.Printf(\"Getting weather for %s\\n\", location)\n            case \"calculate\":\n                a := toolCall.Function.Arguments[\"a\"].(float64)\n                b := toolCall.Function.Arguments[\"b\"].(float64)\n                op := toolCall.Function.Arguments[\"operation\"].(string)\n                // Perform calculation\n                fmt.Printf(\"Calculating %v %s %v\\n\", a, op, b)\n            }\n        }\n    })\n</code></pre>"},{"location":"squawk-tutorial/#complete-function-calling-example","title":"Complete Function Calling Example","text":"<pre><code>toolsList := []llm.Tool{\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"hello\",\n            Description: \"Say hello to a given person with their name\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"name\": {\n                        Type:        \"string\",\n                        Description: \"The name of the person\",\n                    },\n                },\n                Required: []string{\"name\"},\n            },\n        },\n    },\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"addNumbers\",\n            Description: \"Make an addition of the two given numbers\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"a\": {\n                        Type:        \"number\",\n                        Description: \"first operand\",\n                    },\n                    \"b\": {\n                        Type:        \"number\",\n                        Description: \"second operand\",\n                    },\n                },\n                Required: []string{\"a\", \"b\"},\n            },\n        },\n    },\n}\n\nsquawk.New().\n    Model(\"qwen2.5:3b\").\n    Provider(provider.Ollama).\n    Tools(toolsList).\n    User(`say \"hello\" to Bob, say \"hello\" to Sam`).\n    User(`add 2 and 40`).\n    FunctionCalling(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        var results string\n        for _, toolCall := range self.ToolCalls() {\n            switch toolCall.Function.Name {\n            case \"hello\":\n                results += fmt.Sprintf(\"Hello %s\\n\", toolCall.Function.Arguments[\"name\"])\n            case \"addNumbers\":\n                a := toolCall.Function.Arguments[\"a\"]\n                b := toolCall.Function.Arguments[\"b\"]\n                results += fmt.Sprintf(\"Addition of %v and %v is %v\\n\",\n                    a, b, a.(float64)+b.(float64))\n            }\n        }\n        self.System(\"RESULTS:\\n\" + results) // Add results to conversation\n    }).\n    User(\"Use the results and format the output with fancy emojis\").\n    ChatStream(func(answer llm.Answer, self *squawk.Squawk) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n</code></pre>"},{"location":"squawk-tutorial/#additional-features","title":"Additional Features","text":""},{"location":"squawk-tutorial/#provider-configuration","title":"Provider Configuration","text":"<p>Squawk supports multiple providers:</p> <pre><code>// Ollama\nsquawk.New().\n  Model(\"mistral:latest\").\n  BaseURL(\"http://localhost:11434\").\n  Provider(provider.Ollama)\n\n// OpenAI\nsquawk.New().\n  Model(\"gpt-4\").\n  Provider(provider.OpenAI, \"your-api-key\")\n\n// Docker Model Runner\nsquawk.New().\n  Model(\"llama:13b\").\n  BaseURL(\"http://localhost:12434\").\n  Provider(provider.DockerModelRunner)\n</code></pre>"},{"location":"squawk-tutorial/#model-options","title":"Model Options","text":"<p>Configure model behavior:</p> <pre><code>squawk.New().\n  Model(\"mistral:latest\").\n  Provider(provider.Ollama).\n  Options(llm.SetOptions(map[string]interface{}{\n    option.Temperature:   0.7,\n    option.TopP:          0.9,\n    option.RepeatLastN:   64,\n    option.RepeatPenalty: 1.1,\n    option.MaxTokens:     2000,\n  }))\n</code></pre>"},{"location":"squawk-tutorial/#prompting-templates","title":"Prompting Templates","text":"<p>Squawk includes built-in prompting templates:</p> <pre><code>// Explain for kids\nsquawk.New().\n    Model(\"qwen2.5:3b\").\n    Provider(provider.Ollama).\n    ForKids(\"Explain Docker\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        fmt.Println(answer.Message.Content)\n    })\n\n// Brief explanation\nsquawk.New().\n    Model(\"qwen2.5:3b\").\n    Provider(provider.Ollama).\n    Brief(\"Explain Docker\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        fmt.Println(answer.Message.Content)\n    })\n\n\n// Other templates include:\n// - AsAPoem()\n// - AsARecipe()\n// - Timeline()\n// - Comparison()\n// - StepByStep()\n// - ProsAndCons()\n// - InLaymansTerms()\n// - Summarize()\n</code></pre>"},{"location":"squawk-tutorial/#error-handling","title":"Error Handling","text":"<pre><code>sq := squawk.New().\n  Model(\"mistral:latest\").\n  Provider(provider.Ollama).\n  User(\"Generate some code\").\n  Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n    if err != nil {\n      self.LastError(err)  // Set error\n      return\n    }\n  })\n\n// Check for errors\nif err := sq.LastError(); err != nil {\n  fmt.Printf(\"Operation failed: %v\\n\", err)\n}\n</code></pre>"},{"location":"squawk-tutorial/#command-execution","title":"Command Execution","text":"<p>Execute custom commands on a Squawk instance:</p> <pre><code>squawk.New().\n    Model(\"mistral:latest\").\n    Provider(provider.Ollama).\n    System(\"You are a Go expert\").\n    Cmd(func(self *squawk.Squawk) {\n        fmt.Println(self.Messages())\n    })\n</code></pre> <p>Note</p> <p>\ud83d\udc40 you will find complete examples in:</p> <ul> <li>examples/86-squawk-demo</li> <li>examples/87-squawk-meta-prompts</li> <li>examples/88-squawk-embeddings</li> <li>examples/89-squawk-tools</li> <li>examples/92-squawk-tutorial</li> </ul>"},{"location":"structured-outputs/","title":"Structured outputs","text":"<p>\ud83d\udea7 work in progress</p> <p>\ud83d\udcdd Ref: https://ollama.com/blog/structured-outputs</p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/66-structured-outputs</li> </ul>"},{"location":"tools-box/","title":"Tools Box","text":""},{"location":"tools-box/#strings-and-json","title":"Strings and JSON","text":"<ul> <li><code>PrettyString</code> formats a JSON string into a more human-readable format: <code>gear.PrettyString(str string) (string, error)</code></li> <li><code>JSONParse</code> parses a JSON string into a <code>map[string]interface{}</code>: <code>gear.JSONParse(str string) (map[string]interface{}, error)</code></li> <li><code>JSONStringify</code> converts a <code>map[string]interface{}</code> object into a JSON string: <code>gear.JSONStringify(obj map[string]interface{}) string</code></li> </ul>"},{"location":"tools-box/#get-and-cast-environment-variable-value-at-the-same-time","title":"Get and cast environment variable value at the same time:","text":"<ul> <li><code>gear.GetEnvFloat(key string, defaultValue float64) float64</code></li> <li><code>gear.GetEnvInt(key string, defaultValue int) int</code></li> <li><code>gear.GetEnvString(key string, defaultValue string) string</code></li> </ul>"},{"location":"tools/","title":"Function Calling with tool support","text":"<p>Ollama API: chat request with tools https://github.com/ollama/ollama/blob/main/docs/api.md#chat-request-with-tools</p> <p>Since Ollama <code>0.3.0</code>, Ollama supports tools calling, blog post: https://ollama.com/blog/tool-support. A list of supported models can be found under the Tools category on the models page: https://ollama.com/search?c=tools</p>"},{"location":"tools/#define-a-list-of-tools","title":"Define a list of tools","text":"<p>use a supported model</p> <pre><code>model := \"mistral:7b\"\n\ntoolsList := []llm.Tool{\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"hello\",\n            Description: \"Say hello to a given person with his name\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"name\": {\n                        Type:        \"string\",\n                        Description: \"The name of the person\",\n                    },\n                },\n                Required: []string{\"name\"},\n            },\n        },\n    },\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"addNumbers\",\n            Description: \"Make an addition of the two given numbers\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"a\": {\n                        Type:        \"number\",\n                        Description: \"first operand\",\n                    },\n                    \"b\": {\n                        Type:        \"number\",\n                        Description: \"second operand\",\n                    },\n                },\n                Required: []string{\"a\", \"b\"},\n            },\n        },\n    },\n}\n</code></pre>"},{"location":"tools/#set-the-tools-property-of-the-query","title":"Set the Tools property of the query","text":"<ul> <li>set the <code>Temperature</code> to <code>0.0</code></li> <li>you don't need to set the row mode to true</li> <li>set <code>query.Tools</code> with <code>toolsList</code></li> </ul> <pre><code>messages := []llm.Message{\n    {Role: \"user\", Content: `say \"hello\" to Bob`},\n}\n\noptions := llm.SetOptions(map[string]interface{}{\n    option.Temperature: 0.0,\n    option.RepeatLastN: 2,\n    option.RepeatPenalty: 2.0,\n})\n\nquery := llm.Query{\n    Model:    model,\n    Messages: messages,\n    Tools:    toolsList,\n    Options:  options,\n}\n</code></pre>"},{"location":"tools/#run-the-completion","title":"Run the completion","text":"<pre><code>answer, err := completion.Chat(ollamaUrl, query)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n\n// It's a []map[string]interface{}\ntoolCalls := answer.Message.ToolCalls\n\n// Convert toolCalls into a JSON string\njsonBytes, err := json.Marshal(toolCalls)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n// Convert JSON bytes to string\nresult := string(jsonBytes)\n\nfmt.Println(result)\n</code></pre> <p>The result will look like this: <pre><code>[{\"function\":{\"arguments\":{\"name\":\"Bob\"},\"name\":\"hello\"}}]\n</code></pre></p>"},{"location":"tools/#or-you-can-use-the-toolcallstojsonstring-helper","title":"Or you can use the <code>ToolCallsToJSONString</code> helper","text":"<p><pre><code>answer, err = completion.Chat(ollamaUrl, query)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n\nresult, err = answer.Message.ToolCallsToJSONString()\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\nfmt.Println(result)\n</code></pre> The result will look like this: <pre><code>[{\"function\":{\"arguments\":{\"name\":\"Bob\"},\"name\":\"hello\"}}]\n</code></pre></p> <p>Note</p> <p>Look here for a complete sample: examples/19-mistral-function-calling-tool-support</p>"},{"location":"tools/#or-better-you-can-use-the-toolcallstojsonstring-helper","title":"Or (better) you can use the <code>ToolCallsToJSONString</code> helper","text":"<pre><code>answer, err := completion.Chat(ollamaUrl, query)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n\nresult, err := answer.Message.ToolCalls[0].Function.ToJSONString()\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\nfmt.Println(result)\n</code></pre> <p>The result will look like this: <pre><code>{\"name\":\"hello\",\"arguments\":{\"name\":\"Bob\"}}\n</code></pre></p> <p>Note</p> <p>Look at these samples:</p> <ul> <li>examples/43-function-calling/01-xlam</li> <li>examples/43-function-calling/02-qwen2tools</li> </ul>"},{"location":"ui-helpers/","title":"UI helpers","text":"<p>\ud83d\udce6 <code>ui</code> package</p> <p>These helpers provide methods to help you to create a better CLI interface for interacting with the LLMs.</p>"},{"location":"ui-helpers/#input","title":"<code>Input</code>","text":"<p><code>func Input(color, prompt string) (string, error)</code></p> <p><code>Input</code> displays a prompt with the specified color and waits for user input.</p> <p>Parameters:   - color: A string representing the color of the prompt text.   - prompt: A string representing the prompt message to display.</p> <p>Returns:   - A string containing the user input, trimmed of any leading or trailing whitespace.   - An error if there was an issue running the input program or if the input could not be retrieved.</p> <pre><code>res, err := ui.Input(colors.Cyan, \"\ud83e\udd16 ask me something&gt;\")\n</code></pre>"},{"location":"ui-helpers/#println","title":"<code>Println</code>","text":"<p><code>Println(color string, strs ...interface{})</code></p> <p><code>Println</code> prints the provided strings with the specified color using the lipgloss styling library. The color parameter should be a string representing the desired color. The strs parameter is a variadic argument that accepts multiple values to be printed.</p> <p>Parameters:   - color: A string representing the color to be used for the text.   - strs: A variadic parameter that accepts multiple values to be printed.</p> <pre><code>ui.Println(colors.Magenta, \"\ud83d\udc4b hello \ud83e\udd16\")\n</code></pre> <p>Imports: <pre><code>\"github.com/parakeet-nest/parakeet/ui\"\n\"github.com/parakeet-nest/parakeet/ui/colors\"\n</code></pre></p> <p>Info</p> <p>I used these two great libraries to create the helpers: - github.com/charmbracelet/bubbletea - github.com/charmbracelet/lipgloss</p> <p>Note</p> <p>\ud83d\udc40 you will find complete examples in:</p> <ul> <li>examples/58-michael-burnham</li> <li>examples/59-jean-luc-picard-contextual-retrieval</li> </ul>"},{"location":"verbose-mode/","title":"Verbose mode","text":"<p>You can activate the \"verbose mode\" with all kinds of completions.</p> <p><pre><code>options := llm.SetOptions(map[string]interface{}{\n  option.Temperature: 0.5,\n  option.RepeatLastN: 2,\n  option.RepeatPenalty: 2.0,\n  option.Verbose: true,\n})\n</code></pre> You will get an output like this (with the query and the completion):</p> <pre><code>[llm/query] {\n  \"model\": \"deepseek-coder\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are an expert in computer programming.\\n\\tPlease make friendly answer for the noobs.\\n\\tAdd source code examples if you can.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"I need a clear explanation regarding the following question:\\n\\tCan you create a \\\"hello world\\\" program in Golang?\\n\\tAnd, please, be structured with bullet points\"\n    }\n  ],\n  \"options\": {\n    \"repeat_last_n\": 2,\n    \"temperature\": 0.5,\n    \"repeat_penalty\": 2,\n    \"Verbose\": true\n  },\n  \"stream\": false,\n  \"prompt\": \"\",\n  \"context\": null,\n  \"tools\": null,\n  \"TokenHeaderName\": \"\",\n  \"TokenHeaderValue\": \"\"\n}\n\n[llm/completion] {\n  \"model\": \"deepseek-coder\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Sure, here's a simple \\\"Hello, World!\\\" program in Golang.\\n\\t1. First, you need to have Golang installed on your machine.\\n\\t2. Open your text editor, and write the following code:\\n\\t```go\\n\\tpackage main\\n\\timport \\\"fmt\\\"\\n\\tfunc main() {\\n\\t    fmt.Println(\\\"Hello, World!\\\")\\n\\t} \\n\\t```\\n\\t3. Save the file with a `.go` extension (like `hello.go`).\\n\\t4. In your terminal, navigate to the directory containing the `.go` file.\\n\\t5. Run the program with the command:\\n\\t```\\n\\tgo run hello.go\\n\\t```\\n\\t6. If everything goes well, you should see \\\"Hello, World!\\\" printed in your terminal.\\n\\t7. If there's an error, you will see the error message.\\n\\t8. If everything is correct, you'll see \\\"Hello, World!\\\" printed in your terminal.\\n\"\n  },\n  \"done\": true,\n  \"response\": \"\",\n  \"context\": null,\n  \"created_at\": \"2024-08-19T05:57:23.979361Z\",\n  \"total_duration\": 3361191958,\n  \"load_duration\": 2044932125,\n  \"prompt_eval_count\": 79,\n  \"prompt_eval_duration\": 95034000,\n  \"eval_count\": 222,\n  \"eval_duration\": 1216689000\n}\n</code></pre>"},{"location":"wasm-plugins/","title":"\ud83d\udfea WASM Plugins","text":""},{"location":"wasm-plugins/#wasm-plugins","title":"Wasm plugins","text":"<p>The release <code>0.0.6</code> of Parakeet brings the support of WebAssembly thanks to the Extism project. That means you can write your own wasm plugins for Parakeet to add new features (for example, a chunking helper for doing RAG) with various languages (Rust, Go, C, ...).</p> <p>Or you can use the Wasm plugins with the \"Function Calling\" feature, which is implemented in Parakeet.</p> <p>Note</p> <p>You can find an example of \"Wasm Function Calling\" in examples/18-call-functions-for-real - the wasm plugin is located in the <code>wasm</code> folder and it is built with TinyGo.</p> <p>\ud83d\udea7 more samples to come.</p>"},{"location":"what-is-new/","title":"What's new with Parakeet","text":""},{"location":"what-is-new/#parakeet-v029-pie","title":"\ud83e\udd9c Parakeet <code>v0.2.9</code> \ud83e\udd67 [pie]","text":"<ul> <li>Fixed: function calling: no need to use the JSON format with the query options</li> <li>Added: </li> <li>MCP Streamable HTTP Transport support. MCP HTTP client example: <code>95-mcp-http</code></li> <li><code>squawk.SchemaJSON(schemaJSONString string) *Squawk</code></li> </ul>"},{"location":"what-is-new/#parakeet-v028-doughnut","title":"\ud83e\udd9c Parakeet <code>v0.2.8</code> \ud83c\udf69 [doughnut]","text":"<ul> <li>Landing of Squawk: a Parakeet DSL <pre><code>squawk.New().\n    Model(model).\n    BaseURL(ollamaBaseUrl).\n    Provider(provider.Ollama).\n    Options(options).\n    System(\"You are a useful AI agent, you are a Star Trek expert.\").\n    User(\"Who is James T Kirk?\").\n    Chat(func(answer llm.Answer, self *squawk.Squawk, err error) {\n        fmt.Println(answer.Message.Content)\n    })\n</code></pre></li> <li>Improvement of the history messages management</li> <li>Added support for structured output to the Docker Model Runner Chat API</li> <li>Added support for structured output to the OpenAI Chat API</li> </ul>"},{"location":"what-is-new/#parakeet-v027-spouting-whale","title":"\ud83e\udd9c Parakeet <code>v0.2.7</code> \ud83d\udc33 [spouting whale]","text":"<p>Addition of Docker Model Runner support (and OpenAI at the same time) allowing easy development of generative AI applications in Docker containers.</p> <pre><code>modelRunnerURL := \"http://model-runner.docker.internal/engines/llama.cpp/v1/\"\nmodel := \"ai/qwen2.5:latest\" \n\nsystemContent := `You are an expert in Star Trek.`\nuserContent := `Who is Jean-Luc Picard?`\n\noptions := llm.SetOptions(map[string]interface{}{\n    option.Temperature: 0.5,\n    option.RepeatPenalty: 2.0,\n})\n\nquery := llm.Query{\n    Model: model,\n    Messages: []llm.Message{\n        {Role: \"system\", Content: systemContent},\n        {Role: \"user\", Content: userContent},\n    },\n    Options: options,\n}\n\nanswer, err := completion.Chat(\n    modelRunnerURL, \n    query, \n    provider.DockerModelRunner,\n)\nif err != nil {\n    log.Fatal(\"\ud83e\udee2 Oops!\", err)\n}\nfmt.Println(answer.Message.Content)\n</code></pre> <p>Note</p> <p>\ud83d\udc4b Look at this Docker Compose sample examples/82-web-chat-bot-model-runner</p>"}]}